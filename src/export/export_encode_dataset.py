"""
This module provides the `ExportModel` class and supporting methods.
"""
import logging
import os
import numpy as np
import torch
import torch.nn as nn
import h5py
from selene_sdk.utils import (
    load_model_from_state_dict,
)
from tqdm import tqdm
from selene_sdk.utils import initialize_logger

logger = logging.getLogger("selene")


class ExportEncodeDatasetModel(object):
    """
    This class ties together the various objects and methods needed to
    export dataset (partially) processed by a model.

    Parameters
    ----------
    model : torch.nn.Module
        The model to train.
    train_loader: torch.utils.data.DataLoader
        Data loader that fetches batches to train on.
    val_loader: torch.utils.data.DataLoader
        Data loader that fetches validation batches.
    test_loader: torch.utils.data.DataLoader
        Data loader that fetches test batches.
    output_dir : str
        The output directory to save data
    cpu_n_threads : int, optional
        Default is 1. Sets the number of OpenMP threads used for parallelizing
        CPU operations.
    device : str, optional
        Default is `cpu`. Specify a CUDA-device, e.g. 'cuda:2' for on-GPU training.
    data_parallel : bool, optional
        Default is `False`. Specify whether multiple GPUs are available
        for torch to use during training.
    logging_verbosity : {0, 1, 2}, optional
        Default is 2. Set the logging verbosity level.

            * 0 - Only warnings will be logged.
            * 1 - Information and warnings will be logged.
            * 2 - Debug messages, information, and warnings will all be\
                  logged.

    checkpoint_resume : str or None, optional
        Default is `None`. If `checkpoint_resume` is not None, it should be the
        path to a model file generated by `torch.save` that can now be read
        using `torch.load`.
    dnalm_checkpoint_resume : str or None, optional
        Default is `None`. If `dnalm_checkpoint_resume` is not None, it should be the
        path to a model file of dnalm checkpoint. Either checkpoint_resume or dnalm_checkpoint_resume
        should be not `None`

    Attributes
    ----------
    model : torch.nn.Module
        The model to train.
    train_loader : torch.utils.data.DataLoader
        Training data loader.
    val_loader : torch.utils.data.DataLoader
        Validation data loader.
    test_loader: torch.utils.data.DataLoader
        Test data loader.
    masked_targets : bool
        Whether the training dataset generates targets with a mask of existing targets or not
    device : torch.device
        Device on which the computation is carried out.
    data_parallel : bool
        Whether to use multiple GPUs or not.
    output_dir : str
        The directory to save model checkpoints and logs.
    """

    def __init__(
        self,
        model,
        train_loader,
        val_loader,
        test_loader,
        output_dir,
        file_prefix,
        cpu_n_threads=1,
        device="cpu",
        data_parallel=False,
        logging_verbosity=2,
        checkpoint_resume=None,
        dnalm_checkpoint_resume=None,
    ):
        """
        Constructs a new `TrainModel` object.
        """
        self.model = model
        self.file_prefix = file_prefix
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader

        self.batch_size = train_loader.batch_size

        torch.set_num_threads(cpu_n_threads)

        self.data_parallel = data_parallel
        if self.data_parallel:
            self.model = nn.DataParallel(model, device_ids=device, output_device=device[0])
            self.device = self.model.device_ids[0]
            logger.debug("Wrapped model in DataParallel")
        else:
            self.device = torch.device(device)
        self.model.to(self.device)
        logger.debug(f"Set modules to use device {device}")

        os.makedirs(output_dir, exist_ok=True)
        self.output_dir = output_dir
        
        initialize_logger(
            os.path.join(self.output_dir, "{0}.log".format(__name__)),
            verbosity=logging_verbosity,
        )


        if dnalm_checkpoint_resume is not None and checkpoint_resume is not None:
            raise NotImplementedError("Should specify either dnalm_checkpoint_resume or checkpoint_resume, not both of them!")

        if dnalm_checkpoint_resume is None and checkpoint_resume is None:
            raise NotImplementedError("Should specify either dnalm_checkpoint_resume or checkpoint_resume")

        if dnalm_checkpoint_resume is not None:
            logger.info(f"Loading DNALM checkpoint from {dnalm_checkpoint_resume}")
            checkpoint = torch.load(dnalm_checkpoint_resume, map_location='cpu')
            missing_k, unexpected_k = self.model.load_state_dict(checkpoint["model_state_dict"], strict=False)
            if len(missing_k) != 0:
                logger.info(f'{missing_k} were not loaded from checkpoint! These parameters were randomly initialized.')
            if len(unexpected_k) != 0:
                logger.info(f'{unexpected_k} were found in checkpoint, but model is not expecting them!')

        if checkpoint_resume is not None:
            checkpoint = torch.load(
                checkpoint_resume, map_location=lambda storage, location: storage
            )
            if "state_dict" not in checkpoint:
                raise ValueError(
                    "Selene does not support continued "
                    "training of models that were not originally "
                    "trained using Selene."
                )

            self.model = load_model_from_state_dict(
                checkpoint["state_dict"], self.model
            )

    def _export_data(self, data_loader,
                            fname, 
                            fmode="w"):
        """
        Export data obtained from specific data loader
        """
        self.model.eval()

        retrieved_seq, cell_type, target, target_mask = data_loader.dataset.__getitem__(0)
        with h5py.File(fname, fmode) as g:
            target = g.create_dataset("target",
                                      shape=[len(data_loader.dataset)] + list(target.shape),
                                      dtype=target.dtype
                                      )
            g_seq = g.create_group("sequence")

            n_layers = self.model.config.num_hidden_layers
            hidden_size = self.model.config.hidden_size

            hidden_state_representations = ["CLS","MAX","AVERAGE"]
            representations = {}
            for r in hidden_state_representations:
                representations[r] = g_seq.create_dataset(r,
                                      shape=(len(data_loader.dataset), n_layers, hidden_size),
                                      dtype=next(self.model.parameters()).detach().cpu().numpy().dtype,
                                    )
            for batch_id,batch in tqdm(enumerate(data_loader)):
                targets = batch[2]

                batch_st_id = batch_id * self.batch_size 
                batch_en_id = batch_st_id + targets.size()[0]
                target[batch_st_id:batch_en_id, : ] = targets
                
                sequence_batch = {k:v.to(self.device) for k,v in batch[0].items()}

                with torch.no_grad():
                    assert self.model.return_embeddings
                    
                    # tuple of size hidden_size+1
                    outputs = self.model(**sequence_batch)
                    
                    CLS_cat = []
                    AVERAGE_cat = []
                    MAX_cat = []
                    for layer in range(1,len(outputs)): # skip first hidden_state                            
                        # CLS
                        # batch_size x n_tokens x hidden_state_len --> batch_size x 1  x hidden_state_len
                        CLS_cat.append(torch.unsqueeze(outputs[layer][:,0,:],1))

                        masked_hs = (outputs[layer] * sequence_batch["attention_mask"].unsqueeze(dim=2))
                        # size =  torch.Size([batch_size, n_tokens, hiddenst_length])

                        numerator = masked_hs.sum(dim=1)
                        # size = (batch_size, hiddenst_length)

                        denominator = torch.sum(sequence_batch["attention_mask"], dim=1).unsqueeze(dim=1)
                        # denominator.size() = torch.Size([batch_size, 1])
                        masked_average = numerator / denominator
                        masked_average = masked_average.unsqueeze(dim=1)
                        # size = [batch_size, 1, hiddenst_length])
                        
                        AVERAGE_cat.append(masked_average)
                        
                        masked_max = torch.max(masked_hs, dim=1)[0].unsqueeze(dim=1)
                        # size = [batch_size, 1, hiddenst_length])

                        MAX_cat.append(masked_max)
                
                    CLS = torch.cat(CLS_cat,axis=1)
                    MAX = torch.cat(MAX_cat,axis=1)
                    AVERAGE = torch.cat(AVERAGE_cat,axis=1)

                    for r in hidden_state_representations:
                        outputs = eval(r)
                        outputs = outputs.detach().cpu().numpy()
                        assert outputs.shape[1:] == (n_layers, hidden_size), "Array shape error for "+r
                        representations[r][batch_st_id:batch_en_id, : ] = outputs
                        assert representations[r][batch_st_id:batch_en_id, : ].shape == outputs.shape, "Array shape error for "+r

    def export(self):
        """
        Export sequence embeddings and targets.

        Returns
        -------
        None

        """
        self.seq_emb_length = self.model.CLS_embedding_length
        
        for dloader,label in zip([self.test_loader,
                                  self.train_loader,
                                  self.val_loader
                                  ],
                                  ["test","train","validation"]
                                  ):
            output_file = os.path.join(self.output_dir,
                                       "ds_"+label+"_"+self.file_prefix+".hdf5")
            logging.info(f"Exporting data for {label} to file {output_file}")
            self._export_data(dloader,fname = output_file, 
                                        fmode="w", 
                                        )