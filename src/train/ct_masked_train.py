"""
This module provides the `TrainModel` class and supporting methods.
"""
import copy
import logging
import math
import os
import random
import shutil
import warnings
from time import time

import numpy as np
import tensorboard as tb
import torch
import torch.nn as nn
from selene_sdk.utils import (
    PerformanceMetrics,
    initialize_logger,
    load_model_from_state_dict,
)
from sklearn.metrics import (
    ConfusionMatrixDisplay,
    average_precision_score,
    confusion_matrix,
    roc_auc_score,
)
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

warnings.filterwarnings("ignore")


logger = logging.getLogger("selene")


def _metrics_logger(name, out_filepath):
    logger = logging.getLogger("{0}".format(name))
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter("%(message)s")
    file_handle = logging.FileHandler(
        os.path.join(out_filepath, "{0}.txt".format(name))
    )
    file_handle.setFormatter(formatter)
    logger.addHandler(file_handle)
    return logger


class TrainMaskedCTModel(object):
    """
    This class ties together the various objects and methods needed to
    train and validate a model on cell type / sequence folds.

    TrainMaskedCTModel saves a checkpoint model (overwriting it after
    each fold) as well as a best-performing model (overwriting it after
    each fold if the latest validation performance is better than the previous
    best-performing model) to `output_dir`.

    TrainMaskedCTModel also outputs 2 files that can be used to monitor training
    as Selene runs: `selene_sdk.train_model.train.txt` (training loss & metrics) and
    `selene_sdk.train_model.validation.txt` (validation loss & metrics).
    The columns in these files can be used to quickly visualize
    training history (e.g. you can use `matplotlib`, `plt.plot(auc_list)`)
    and see, for example, whether the model is still improving, if there are
    signs of overfitting, etc.

    Parameters
    ----------

        score_threshold=0.5,
        save_track_metrics_during_training=True,

    model : torch.nn.Module
        The model to train.
    ct_masks : numpy.array
        Array of cell type indices to exclude from each fold.
    n_cell_types : int
        Number of cell types.
    loss_criterion : torch.nn._Loss
        The loss function to optimize.
    optimizer_class : torch.optim.Optimizer
        The optimizer to minimize loss with.
    optimizer_kwargs : dict
        The dictionary of keyword arguments to pass to the optimizer's
        constructor.
    dataloaders : list(tuple(torch.utils.data.DataLoader))
        List of tuples of (train_loader, validation_loader) for each fold.
    n_epochs : int
        The maximum number of epochs to iterate over the dataset.
    checkpoint_epoch : int
        Epoch to start training from (if checkpoint is provided).
    checkpoint_chunk : int
        Fold to start training from (if checkpoint is provided).
    checkpoint_resume : str or None
        If `checkpoint_resume` is not None, it should be the
        path to a model file generated by `torch.save` that can now be read
        using `torch.load`.
    output_dir : str
        The output directory to save model checkpoints and logs in.
    scheduler_class : torch.optim.lr_scheduler, optional
        The LR scheduler class to use with specified optimizer.
    scheduler_kwargs : dict, optional
        The dictionary of keyword arguments to pass to the LR scheduler's
        constructor.
    report_gt_feature_n_positives : int
        Minimum number of positive targets to compute metrics.
    cpu_n_threads : int, optional
        Default is 1. Sets the number of OpenMP threads used for parallelizing
        CPU operations.
    device : str, optional
        Default is `cpu`. Specify a CUDA-device, e.g. 'cuda:2' for on-GPU training.
    data_parallel : bool, optional
        Default is `False`. Specify whether multiple GPUs are available
        for torch to use during training.
    logging_verbosity : {0, 1, 2}, optional
        Default is 2. Set the logging verbosity level.

            * 0 - Only warnings will be logged.
            * 1 - Information and warnings will be logged.
            * 2 - Debug messages, information, and warnings will all be\
                  logged.

    metrics : dict(metric_name: metric_fn)
        Default is `dict(roc_auc=roc_auc_score, average_precision=average_precision_score)`.
        Metric functions to log.
    metrics_transforms : dict(metric_name: transform_fn)
        Default is `dict(roc_auc=None, average_precision=None)`.
        Transforms to apply before computing the metrics.
    log_confusion_matrix : bool, optional
        Default is `True`. Specify whether confusion matrix should be logged.
    score_threshold : int, optional
        Default is 0.5. Score threshold to determine prediction based on the model output score.
    save_track_metrics_during_training : bool
        Default is `True`. Save per-track metrics in addition to average metric values.


    Attributes
    ----------
    model : torch.nn.Module
        The model to train.
    loss_criterion : torch.nn._Loss
        The loss function to optimize.
    optimizer : torch.optim.Optimizer
        The optimizer to minimize loss with.
    scheduler : torch.optim.lr_scheduler
        The LR scheduler to use with optimizer.
    dataloaders : list(tuple(torch.utils.data.DataLoader))
        List of tuples of (train_loader, validation_loader) for each fold.
    masked_targets : bool
        Whether the training dataset generates targets with a mask of existing targets or not
    n_epochs : int
        The maximum number of epochs to iterate over the dataset.
    device : torch.device
        Device on which the computation is carried out.
    data_parallel : bool
        Whether to use multiple GPUs or not.
    output_dir : str
        The directory to save model checkpoints and logs.
    training_loss : list(float)
        The current training loss.
    metrics : dict
        A dictionary that maps metric names (`str`) to metric functions.
        By default, this contains `"roc_auc"`, which maps to
        `sklearn.metrics.roc_auc_score`, and `"average_precision"`,
        which maps to `sklearn.metrics.average_precision_score`.
    current_ct_mask : np.array(bool)
        Mask of cell types of a fold currently in processing.

    """

    def __init__(
        self,
        model,
        ct_masks,
        n_cell_types,
        loss_criterion,
        optimizer_class,
        optimizer_kwargs,
        dataloaders,
        n_epochs,
        checkpoint_epoch,
        checkpoint_chunk,
        checkpoint_resume,
        output_dir,
        scheduler_class=None,
        scheduler_kwargs=None,
        report_gt_feature_n_positives=10,
        cpu_n_threads=1,
        device="cpu",
        data_parallel=False,
        logging_verbosity=2,
        metrics=dict(roc_auc=roc_auc_score, average_precision=average_precision_score),
        metrics_transforms=dict(roc_auc=None, average_precision=None),
        log_confusion_matrix=True,
        score_threshold=0.5,
        save_track_metrics_during_training=True,
    ):
        """
        Constructs a new `TrainModel` object.
        """
        self.model = model
        self.checkpoint_resume = checkpoint_resume
        self.checkpoint_epoch = checkpoint_epoch
        self.checkpoint_chunk = checkpoint_chunk
        self.n_cell_types = n_cell_types
        self.ct_masks = ct_masks
        self.dataloaders = dataloaders
        n_epoch_steps = sum(
            [len(train_loader) for train_loader, val_loader in self.dataloaders]
        )
        print(f"Total epoch steps: {n_epoch_steps}")
        self.criterion = loss_criterion
        self.optimizer = optimizer_class(self.model.parameters(), **optimizer_kwargs)

        if scheduler_class is not None:
            if scheduler_kwargs is None:
                scheduler_kwargs = dict()
            self.scheduler = scheduler_class(self.optimizer, **scheduler_kwargs)

        self.masked_targets = True
        self.n_epochs = n_epochs

        logger.info(
            "Training parameters set: batch size {0}, "
            "number of epochs: {1}".format(dataloaders[0][0].batch_size, self.n_epochs)
        )

        torch.set_num_threads(cpu_n_threads)

        self.device = torch.device(device)
        self.data_parallel = data_parallel

        if self.data_parallel:
            self.model = nn.DataParallel(model)
            logger.debug("Wrapped model in DataParallel")
        else:
            self.model.to(self.device)
            self.criterion.to(self.device)
            logger.debug(f"Set modules to use device {device}")

        os.makedirs(output_dir, exist_ok=True)
        self.output_dir = output_dir
        self._writer = SummaryWriter(os.path.join(self.output_dir))

        initialize_logger(
            os.path.join(self.output_dir, "{0}.log".format(__name__)),
            verbosity=logging_verbosity,
        )

        self._validation_metrics = PerformanceMetrics(
            lambda idx: self.dataloaders[0][0].dataset.target_features[idx],
            lambda idx: self.dataloaders[0][0].dataset._cell_types[idx],
            report_gt_feature_n_positives=report_gt_feature_n_positives,
            metrics=metrics,
            metrics_transforms=metrics_transforms,
        )

        # create separate baseline metrics object without target sigmoid transformation
        first_dataset = self.dataloaders[0][0].dataset
        self.quantitative_target = False
        if (
            hasattr(first_dataset, "quantitative_features")
            and first_dataset.quantitative_features
        ) or (
            hasattr(first_dataset, "target_class")
            and first_dataset.target_class.__name__ == "qGenomicFeatures"
        ):
            self.quantitative_target = True
        if self.quantitative_target:
            baseline_metrics_transforms = {}
            for m, t in metrics_transforms.items():
                if t.__class__.__name__ == "Compose":
                    baseline_ts = copy.copy(t)
                    baseline_ts.transforms = [
                        tr
                        for tr in baseline_ts.transforms
                        if tr.__class__.__name__ != "Quantitative2Sigmoid"
                    ]
                    baseline_metrics_transforms[m] = baseline_ts
        else:
            baseline_metrics_transforms = metrics_transforms

        self._baseline_validation_metrics = PerformanceMetrics(
            lambda idx: self.dataloaders[0][0].dataset.target_features[idx],
            lambda idx: self.dataloaders[0][0].dataset._cell_types[idx],
            report_gt_feature_n_positives=report_gt_feature_n_positives,
            metrics=metrics,
            metrics_transforms=baseline_metrics_transforms,
        )
        self._test_metrics = PerformanceMetrics(
            lambda idx: self.dataloaders[0][0].dataset.target_features[idx],
            lambda idx: self.dataloaders[0][0].sberbankdataset._cell_types[idx],
            report_gt_feature_n_positives=report_gt_feature_n_positives,
            metrics=metrics,
            metrics_transforms=metrics_transforms,
        )
        self.log_confusion_matrix = log_confusion_matrix
        self.save_track_metrics_during_training = save_track_metrics_during_training

        self._start_step = 0
        # TODO: Should this be set when it is used later? Would need to if we want to
        # train model 2x in one run.
        self._min_loss = float("inf")

        if checkpoint_resume is not None:
            checkpoint = torch.load(
                checkpoint_resume, map_location=lambda storage, location: storage
            )
            if "state_dict" not in checkpoint:
                raise ValueError(
                    "Selene does not support continued "
                    "training of models that were not originally "
                    "trained using Selene."
                )

            self.model = load_model_from_state_dict(
                checkpoint["state_dict"], self.model
            )

            self._start_step = checkpoint["step"]
            # if self._start_step >= self.n_epochs:
            #    self.n_epochs += self._start_step

            self._min_loss = checkpoint["min_loss"]
            self.optimizer.load_state_dict(checkpoint["optimizer"])
            for state in self.optimizer.state.values():
                for k, v in state.items():
                    if isinstance(v, torch.Tensor):
                        state[k] = v.to(self.device)

            logger.info(
                ("Resuming from checkpoint: step {0}, min loss {1}").format(
                    self._start_step, self._min_loss
                )
            )

        self._train_logger = _metrics_logger(
            "{0}.train".format(__name__), self.output_dir
        )
        self._validation_logger = _metrics_logger(
            "{0}.validation".format(__name__), self.output_dir
        )

        self._train_logger.info("loss")
        self._validation_logger.info(
            "\t".join(
                ["loss"]
                + sorted(
                    [x for x in self._validation_metrics.metrics.keys()]
                    + ["baseline_loss"]
                    + sorted(
                        [x for x in self._baseline_validation_metrics.metrics.keys()]
                    )
                )
            )
        )

        self.score_threshold = score_threshold

    def run_masked_train(self):

        min_loss = self._min_loss

        time_per_batch = []
        report_train_losses = []
        report_train_predictions = []
        report_train_targets = []
        report_train_target_masks = []

        total_steps = self._start_step

        # make necessary LR scheduler steps
        # if they are not based on validation loss
        for step in range(1, total_steps + 1):
            self._update_and_log_lr_if_needed(step, log=False)

        time_per_batch = []
        report_train_losses = []
        report_train_predictions = []
        report_train_targets = []
        report_train_target_masks = []

        # get {dataloaders: masks} dict
        self.fold_map = {}
        for i in range(len(self.dataloaders)):
            self.fold_map[self.dataloaders[i]] = self.ct_masks[i]

        if (
            self.checkpoint_resume is not None
            and self.checkpoint_chunk > 0
            and self.n_epochs > 0
        ):

            print(
                f"Start training from {self.checkpoint_epoch} epoch, chunk {self.checkpoint_chunk}"
            )

            # shuffle loaders
            random.seed(666 + self.checkpoint_epoch)
            l = list(self.fold_map.items())
            for chunk, (
                (train_batch_loader, valid_batch_loader),
                current_ct_mask,
            ) in enumerate(dict(random.sample(l, len(self.fold_map.keys()))).items()):

                if chunk >= self.checkpoint_chunk:
                    print("epoch:", self.checkpoint_epoch, "data chunk:", chunk)

                    self.current_ct_mask = self.ct_indices_to_mask(current_ct_mask)

                    # train
                    for batch in tqdm(train_batch_loader):
                        t_i = time()
                        prediction, target, target_mask, loss = self.train(batch)
                        t_f = time()
                        time_per_batch.append(t_f - t_i)
                        report_train_losses.append(loss)
                        report_train_predictions.append(prediction)
                        report_train_targets.append(target)
                        if self.masked_targets:
                            report_train_target_masks.append(target_mask)
                        total_steps += 1
                        self._update_and_log_lr_if_needed(total_steps)

                    # метрики на train для отчета
                    self._log_train_metrics_and_clean_cache(
                        self.checkpoint_epoch,
                        total_steps,
                        time_per_batch,
                        report_train_losses,
                        report_train_predictions,
                        report_train_targets,
                        report_train_target_masks,
                    )
                    time_per_batch = []
                    report_train_losses = []
                    report_train_predictions = []
                    report_train_targets = []
                    report_train_target_masks = []

                    # val
                    validation_loss = self._validate_and_log_metrics(
                        val_loader=valid_batch_loader, step=total_steps
                    )

                    self._update_and_log_lr_if_needed(
                        total_steps, math.ceil(validation_loss * 1000.0) / 1000.0
                    )

                    if validation_loss < min_loss:
                        min_loss = validation_loss
                        self._save_checkpoint(total_steps, min_loss, is_best=True)
                        logger.info("Updating `best_model.pth.tar`")

                    self._writer.add_scalar("data chunk", chunk, total_steps)

                checkpoint_basename = "checkpoint"
                self._save_checkpoint(
                    total_steps,
                    min_loss,
                    is_best=False,
                    filename=checkpoint_basename,
                )
                logger.debug(
                    "Saving checkpoint `{0}.pth.tar`".format(checkpoint_basename)
                )

            self._log_embeddings(total_steps)

            self.checkpoint_epoch += 1
            print(
                f"Epoch from checkpoint completed; Start {self.checkpoint_epoch} epoch"
            )

        # main train/val loop
        for epoch in tqdm(range(self.checkpoint_epoch, self.n_epochs)):

            # shuffle loaders
            random.seed(666 + epoch)
            l = list(self.fold_map.items())
            for chunk, (
                (train_batch_loader, valid_batch_loader),
                current_ct_mask,
            ) in enumerate(dict(random.sample(l, len(self.fold_map.keys()))).items()):

                self.current_ct_mask = self.ct_indices_to_mask(current_ct_mask)

                print("epoch:", epoch, "data chunk:", chunk)

                # train
                for batch in tqdm(train_batch_loader):
                    t_i = time()
                    prediction, target, target_mask, loss = self.train(batch)
                    t_f = time()
                    time_per_batch.append(t_f - t_i)
                    report_train_losses.append(loss)
                    report_train_predictions.append(prediction)
                    report_train_targets.append(target)
                    if self.masked_targets:
                        report_train_target_masks.append(target_mask)
                    total_steps += 1
                    self._update_and_log_lr_if_needed(total_steps)

                # метрики на train для отчета
                self._log_train_metrics_and_clean_cache(
                    epoch,
                    total_steps,
                    time_per_batch,
                    report_train_losses,
                    report_train_predictions,
                    report_train_targets,
                    report_train_target_masks,
                )
                time_per_batch = []
                report_train_losses = []
                report_train_predictions = []
                report_train_targets = []
                report_train_target_masks = []

                # val
                validation_loss = self._validate_and_log_metrics(
                    val_loader=valid_batch_loader, step=total_steps
                )

                self._update_and_log_lr_if_needed(
                    total_steps, math.ceil(validation_loss * 1000.0) / 1000.0
                )

                if validation_loss < min_loss:
                    min_loss = validation_loss
                    self._save_checkpoint(total_steps, min_loss, is_best=True)
                    logger.info("Updating `best_model.pth.tar`")

                self._writer.add_scalar("data chunk", chunk, total_steps)

                checkpoint_basename = "checkpoint"
                self._save_checkpoint(
                    total_steps,
                    min_loss,
                    is_best=False,
                    filename=checkpoint_basename,
                )
                logger.debug(
                    "Saving checkpoint `{0}.pth.tar`".format(checkpoint_basename)
                )

            self._log_embeddings(total_steps)
        self._writer.flush()

        valid_data = []
        for (train_batch_loader, valid_batch_loader), ct_mask in self.fold_map.items():
            valid_data.append((valid_batch_loader, ct_mask))
        self._final_validate(valid_data)
        return None

    def ct_indices_to_mask(self, ct_indices):
        """
        Converts a list of cell type indices into a binary mask.
        """
        ct_mask = np.full(self.n_cell_types, False)
        ct_mask[ct_indices] = True
        return ct_mask

    def train(self, batch):
        """
        Trains the model on a batch of data.

        Returns
        -------
        float
            The training loss.

        """
        self.model.train()

        if self.masked_targets:
            # retrieved_seq, cell_type, target, target_mask
            sequence_batch = batch[0].to(self.device)
            cell_type_batch = batch[1].to(self.device)
            targets = batch[2].to(self.device)
            target_mask = batch[3].to(self.device)

            # make train mask
            target_mask_tr = target_mask.clone()
            target_mask_tr[:, self.current_ct_mask, :] = False

            outputs = self.model(sequence_batch, cell_type_batch)

            self.criterion.weight = target_mask_tr
        else:
            # retrieved_seq, target
            sequence_batch = batch[0].to(self.device)
            targets = batch[1].to(self.device)
            outputs = self.model(sequence_batch)

        loss = self.criterion(outputs, targets)

        if self.criterion.reduction == "sum":
            loss = loss / self.criterion.weight.sum()
        # predictions = torch.sigmoid(outputs)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if self.masked_targets:
            target_mask = target_mask_tr.cpu().detach()
        else:
            target_mask = None

        return (
            outputs.cpu().detach(),
            targets.cpu().detach(),
            target_mask,
            loss.item(),
        )

    def _evaluate_on_ct(self, data_loader):
        """
        Makes predictions for some labeled input data.

        Parameters
        ----------
        data_in_batches : list(SamplesBatch)
            A list of tuples of the data, where the first element is
            the example, and the second element is the label.

        Returns
        -------
        tuple(float, list(torch.tensor))
            Returns the average loss, and the list of all predictions.

        """
        self.model.eval()

        batch_losses = []
        all_predictions = []
        all_targets = []
        all_baselines = []
        if self.masked_targets:
            all_target_masks = []
        else:
            all_target_masks = None

        for batch in tqdm(data_loader):
            sequence_batch = batch[0].to(self.device)
            cell_type_batch = batch[1].to(self.device)
            targets = batch[2].to(self.device)
            target_mask = batch[3].to(self.device)

            # val mask
            target_mask_tr = target_mask.clone()
            target_mask_tr[:, self.current_ct_mask, :] = False
            target_mask_val = target_mask.clone()
            target_mask_val[:, ~self.current_ct_mask, :] = False

            # compute a baseline
            if self.quantitative_target:
                baseline = torch.zeros(
                    targets.shape[0],
                    targets.shape[1] + 1,
                    targets.shape[2],
                    device=targets.device,
                )
                baseline[:, -1, :] += (targets * target_mask_tr).sum(
                    axis=1
                ) / target_mask_tr.sum(axis=1)
            else:
                baseline = (targets * target_mask_tr).sum(axis=1) / target_mask_tr.sum(
                    axis=1
                )
                baseline = torch.repeat_interleave(
                    baseline.unsqueeze(1), self.n_cell_types, dim=1
                )

            with torch.no_grad():
                if self.masked_targets:
                    outputs = self.model(sequence_batch, cell_type_batch)
                    self.criterion.weight = target_mask_val
                else:
                    outputs = self.model(sequence_batch)

                loss = self.criterion(outputs, targets)
                if self.criterion.reduction == "sum":
                    loss = loss / self.criterion.weight.sum()

                # predictions = torch.sigmoid(outputs)

                # baseline_loss = self.criterion(baseline, targets)
                # if self.criterion.reduction == "sum":
                #    baseline_loss = baseline_loss / self.criterion.weight.sum()

                all_predictions.append(outputs.data.cpu())
                all_targets.append(targets.data.cpu())
                all_baselines.append(baseline.data.cpu())
                if self.masked_targets:
                    all_target_masks.append(target_mask.data.cpu())
                batch_losses.append(loss.item())

        return (
            np.average(batch_losses),
            all_predictions,
            all_targets,
            all_baselines,
            all_target_masks,
        )

    def _update_and_log_lr_if_needed(self, total_steps, validation_loss=None, log=True):
        # torch.optim.lr_scheduler.ReduceLROnPlateau is the only scheduler
        # that takes some value as input to `.step()`
        if self.scheduler is not None:
            if validation_loss is None and not isinstance(
                self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau
            ):
                self.scheduler.step()
                if log:
                    self._log_lr(total_steps)
            elif validation_loss is not None and isinstance(
                self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau
            ):
                self.scheduler.step(validation_loss)
                if log:
                    self._log_lr(total_steps)

    def _log_train_metrics_and_clean_cache(
        self,
        epoch,
        step,
        time_per_batch,
        train_losses,
        train_predictions,
        train_targets,
        train_target_masks,
    ):
        logger.info(
            (
                "[epoch {0} / step {1}] average number " "of steps per second: {2:.1f}"
            ).format(epoch, step, 1.0 / np.average(time_per_batch))
        )

        train_loss = np.average(train_losses)
        self._train_logger.info(train_loss)
        self._writer.add_scalar("loss/train", train_loss, step)

        train_scores = self._compute_metrics(
            train_predictions,
            train_targets,
            train_target_masks,
            log_prefix="train",
        )

        for k in sorted(self._validation_metrics.metrics.keys()):
            if k in train_scores and train_scores[k]:
                self._writer.add_scalar(f"model_{k}/train", train_scores[k], step)

        if self.save_track_metrics_during_training:
            self._validation_metrics.write_feature_scores_to_file(
                os.path.join(self.output_dir, str(step) + "_train_metrics.txt")
            )

        logger.info("training loss: {0}".format(train_loss))

    def _validate_and_log_metrics(self, val_loader, step):
        (
            valid_scores,
            baselines_scores,
            all_predictions,
            all_targets,
            all_target_masks,
        ) = self.validate(val_loader)

        validation_loss = valid_scores["loss"]
        self._writer.add_scalar("loss/test", validation_loss, step)
        to_log = [str(validation_loss)]

        # log model metrics
        for k in sorted(self._validation_metrics.metrics.keys()):
            if k in valid_scores and valid_scores[k]:
                to_log.append(str(valid_scores[k]))
                self._writer.add_scalar(f"model_{k}/val", valid_scores[k], step)
            else:
                to_log.append("NA")

        if "loss" in baselines_scores:
            self._writer.add_scalar(
                "baseline_loss/test", baselines_scores["loss"], step
            )
            to_log.append(str(baselines_scores["loss"]))

        # log baseline metrics
        for k in sorted(self._baseline_validation_metrics.metrics.keys()):
            if k in baselines_scores and baselines_scores[k]:
                to_log.append(str(baselines_scores[k]))
                self._writer.add_scalar(f"baseline_{k}/val", baselines_scores[k], step)
            else:
                to_log.append("NA")

        self._validation_logger.info("\t".join(to_log))

        logger.info("validation loss: {0}".format(validation_loss))

        if self.save_track_metrics_during_training:
            self._validation_metrics.write_feature_scores_to_file(
                os.path.join(self.output_dir, str(step) + "_val_metrics.txt")
            )
            self._baseline_validation_metrics.write_feature_scores_to_file(
                os.path.join(self.output_dir, str(step) + "_val_baseline_metrics.txt")
            )

        if self.log_confusion_matrix:
            if self.masked_targets:
                masked_targets = all_targets.flatten()[all_target_masks.flatten()]
                masked_predictions = all_predictions.flatten()[
                    all_target_masks.flatten()
                ]
                cm = confusion_matrix(
                    masked_targets, masked_predictions > self.score_threshold
                )
            else:
                cm = confusion_matrix(
                    all_targets.flatten(),
                    all_predictions.flatten() > self.score_threshold,
                )
            cm_plot = ConfusionMatrixDisplay(confusion_matrix=cm)
            cm_plot.plot()
            self._writer.add_figure(
                "confusion_matrix", cm_plot.figure_, global_step=step
            )

        return validation_loss

    def _compute_metrics(self, predictions, targets, target_mask, log_prefix=None):
        """
        Measures performance on given predictions and targets.

        Returns
        -------
        dict
            A dictionary, where keys are the names of the loss metrics,
            and the values are the average value for that metric over
            the validation set.

        """
        # TODO(arlapin): Should use _train_metrics for "train"?
        scores = self._validation_metrics.update(predictions, targets, target_mask)
        if log_prefix:
            for name, score in scores.items():
                logger.info(f"{log_prefix} {name}: {score}")

        return scores

    def _compute_baseline_score(self, baseline, targets, target_mask, log_prefix=None):
        """
        Measures performance on given predictions and targets.

        Returns
        -------
        dict
            A dictionary, where keys are the names of the loss metrics,
            and the values are the average value for that metric over
            the validation set.

        """

        scores = self._baseline_validation_metrics.update(
            baseline, targets, target_mask
        )
        if log_prefix:
            for name, score in scores.items():
                logger.info(f"{log_prefix} baseline {name}: {score}")

        return scores

    def validate(self, val_loader):
        """
        Measures model validation performance.

        Returns
        -------
        dict
            A dictionary, where keys are the names of the loss metrics,
            and the values are the average value for that metric over
            the validation set.

        """
        (
            average_loss,
            all_predictions,
            all_targets,
            baseline,
            all_target_masks,
        ) = self._evaluate_on_ct(val_loader)

        average_scores = self._compute_metrics(
            all_predictions, all_targets, all_target_masks, log_prefix="validation"
        )
        baselines_scores = self._compute_baseline_score(
            baseline, all_targets, all_target_masks, log_prefix="validation"
        )
        average_scores["loss"] = average_loss

        return (
            average_scores,
            baselines_scores,
            all_predictions,
            all_targets,
            all_target_masks,
        )

    def _save_checkpoint(self, step, min_loss, is_best, filename="checkpoint"):
        """
        Saves snapshot of the model state to file. Will save a checkpoint
        with name `<filename>.pth.tar` and, if this is the model's best
        performance so far, will save the state to a `best_model.pth.tar`
        file as well.
        Models are saved in the state dictionary format. This is a more
        stable format compared to saving the whole model (which is another
        option supported by PyTorch). Note that we do save a number of
        additional, Selene-specific parameters in the dictionary
        and that the actual `model.state_dict()` is stored in the `state_dict`
        key of the dictionary loaded by `torch.load`.
        See: https://pytorch.org/docs/stable/notes/serialization.html for more
        information about how models are saved in PyTorch.
        Parameters
        ----------
        state : dict
            Information about the state of the model. Note that this is
            not `model.state_dict()`, but rather, a dictionary containing
            keys that can be used for continued training in Selene
            _in addition_ to a key `state_dict` that contains
            `model.state_dict()`.
        is_best : bool
            Is this the model's best performance so far?
        filename : str, optional
            Default is "checkpoint". Specify the checkpoint filename. Will
            append a file extension to the end of the `filename`
            (e.g. `checkpoint.pth.tar`).
        Returns
        -------
        None
        """
        state = {
            "step": step,
            "arch": self.model.__class__.__name__,
            "state_dict": self.model.state_dict(),
            "min_loss": min_loss,
            "optimizer": self.optimizer.state_dict(),
        }

        logger.debug("[TRAIN] {0}: Saving model state to file.".format(state["step"]))
        cp_filepath = os.path.join(self.output_dir, filename)
        torch.save(state, "{0}.pth.tar".format(cp_filepath))
        if is_best:
            best_filepath = os.path.join(self.output_dir, "best_model")
            shutil.copyfile(
                "{0}.pth.tar".format(cp_filepath), "{0}.pth.tar".format(best_filepath)
            )

    def _log_lr(self, step):
        lrs = [group["lr"] for group in self.optimizer.param_groups]
        for index, lr in enumerate(lrs):
            self._writer.add_scalar("lr_{}".format(index), lr, step)

    def _log_embeddings(self, step):
        embeddings = self.model.get_cell_type_embeddings()
        cell_type_labels = self.dataloaders[0][0].dataset._cell_types
        self._writer.add_embedding(
            embeddings, metadata=cell_type_labels, global_step=step
        )

    def _final_validate(self, val_data):
        """
        Makes predictions for all validation data and saves it as a final score

        Parameters
        ----------
        val_data : list(tuple(torch.utils.data.DataLoader, numpy.array))
            A list of tuples of validation dataloaders and their corresponding
            cell type masks.

        Returns
        -------
        None
            Writes validation scores using `self._writer` and `logger`.

        """
        all_results = None
        target_cnt = [0 for i in range(len(val_data))]

        for i, (val_loader, ct_mask) in enumerate(val_data):
            self.current_ct_mask = ct_mask
            for batch in val_loader:
                target_cnt[i] = target_cnt[i] + batch[2].sum(axis=0)
            loader_results = self._evaluate_on_ct(val_loader)

            if all_results is None:
                all_results = [[] for i in range(len(loader_results))]

            for loader_res, all_res in zip(loader_results, all_results):
                if isinstance(loader_res, list):
                    all_res.extend(loader_res)
                else:
                    all_res.append(loader_res)
            del loader_results
        target_cnt = torch.stack(target_cnt)
        # import pdb; pdb.set_trace()
        (
            all_losses,
            all_predictions,
            all_targets,
            all_baselines,
            all_target_masks,
        ) = all_results

        valid_scores = self._compute_metrics(
            all_predictions, all_targets, all_target_masks, log_prefix="validation"
        )
        baselines_scores = self._compute_baseline_score(
            all_baselines, all_targets, all_target_masks, log_prefix="validation"
        )
        valid_scores["loss"] = sum(all_losses) / len(all_losses)

        validation_loss = valid_scores["loss"]
        self._writer.add_scalar("loss/final_val", validation_loss)
        to_log = [str(validation_loss)]

        # log model metrics
        for k in sorted(self._validation_metrics.metrics.keys()):
            if k in valid_scores and valid_scores[k]:
                to_log.append(str(valid_scores[k]))
                self._writer.add_scalar(f"model_{k}/final_val", valid_scores[k])
            else:
                to_log.append("NA")

        # log baseline metrics
        for k in sorted(self._baseline_validation_metrics.metrics.keys()):
            if k in baselines_scores and baselines_scores[k]:
                to_log.append(str(baselines_scores[k]))
                self._writer.add_scalar(f"baseline_{k}/final_val", baselines_scores[k])
            else:
                to_log.append("NA")

        self._validation_logger.info("\t".join(to_log))

        logger.info("validation loss: {0}".format(validation_loss))

        if self.save_track_metrics_during_training:
            self._validation_metrics.write_feature_scores_to_file(
                os.path.join(self.output_dir, "final_val_metrics.txt")
            )
            self._baseline_validation_metrics.write_feature_scores_to_file(
                os.path.join(self.output_dir, "final_val_baseline_metrics.txt")
            )
