{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4cde1e9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check free GPUs:\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeef910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Make sure the GPU is enabled \n",
    "assert tf.config.list_physical_devices('GPU')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU') # Set GPUs to use\n",
    "\n",
    "tf.random.set_seed(1) # Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2865f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sonnet as snt\n",
    "from tqdm import tqdm, trange\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import selene_sdk\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import csv\n",
    "import torch\n",
    "import math\n",
    "\n",
    "import sklearn.metrics\n",
    "import scipy.stats\n",
    "import scipy.stats\n",
    "\n",
    "assert snt.__version__.startswith('2.0')\n",
    "\n",
    "from filesampler import RPKMFileSampler\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a470cf",
   "metadata": {},
   "source": [
    "# Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RPKMFileSampler(\n",
    "        '/home/evmalkin/DeepCT/src/data/cross_validation/fold1/train_data_rpkm_log.bed',\n",
    "        reference_sequence = selene_sdk.sequences.Genome('/mnt/datasets/DeepCT/male.hg19.fasta'),\n",
    "        n_cell_types=39,\n",
    "        sequence_length=196_608,\n",
    "        # sequence_length = 115200, # works with attention pooling\n",
    "        balance=False,\n",
    "        zero_expression=None,\n",
    "        keep_zero_percent=None)\n",
    "\n",
    "validation_sampler = RPKMFileSampler(\n",
    "        '/home/evmalkin/DeepCT/src/data/cross_validation/fold1_val/validate_data_rpkm_log.bed',\n",
    "        reference_sequence = selene_sdk.sequences.Genome('/mnt/datasets/DeepCT/male.hg19.fasta'),\n",
    "        n_cell_types=10,\n",
    "        sequence_length=196_608,\n",
    "        # sequence_length = 115200, # works with attention pooling\n",
    "        balance=False,\n",
    "        zero_expression=None,\n",
    "        keep_zero_percent=None)\n",
    "\n",
    "n_validation_samples = 5781\n",
    "train_embeddings = get_embeddings('/home/evmalkin/DeepCT/src/data/cross_validation/fold1/selected_feature_embeddings_len5.csv')\n",
    "validation_embeddings = get_embeddings('/home/evmalkin/DeepCT/src/data/cross_validation/fold1_val/selected_feature_embeddings_len5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6320bf",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'MSE' # 'MSE' or 'correlation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf906daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(targets, predictions, evaluate=False, num_average=300):\n",
    "    if loss_type == 'MSE':\n",
    "        return tf.reduce_mean(tf.keras.losses.MSE(tf.squeeze(targets), tf.squeeze(predictions)))\n",
    "    elif loss_type == 'correlation':\n",
    "        if not evaluate:\n",
    "            avg_correlation = np.mean(prev_correlations_train[-num_average:], dtype='float32')\n",
    "            loss, correlation = correlation_loss(targets, predictions, avg_correlation)\n",
    "            prev_correlations_train.append(correlation)\n",
    "        else:\n",
    "            avg_correlation = np.mean(prev_correlations_validate[-num_average:], dtype='float32')\n",
    "            loss, correlation = correlation_loss(targets, predictions, avg_correlation)\n",
    "            prev_correlations_validate.append(correlation)\n",
    "        return loss\n",
    "\n",
    "prev_correlations_train = [0]\n",
    "prev_correlations_validate = [0]     \n",
    "def correlation_loss(targets, predictions, prev_correlation):\n",
    "    x = tf.squeeze(predictions)\n",
    "    y = tf.squeeze(targets)\n",
    "\n",
    "    vx = x - tf.math.reduce_mean(x)\n",
    "    vy = y - tf.math.reduce_mean(y)\n",
    "    \n",
    "    vary = tf.math.reduce_sum(vy ** 2)\n",
    "    \n",
    "    if vary > 0:\n",
    "        correlation = tf.math.reduce_sum(vx * vy) / (tf.math.sqrt(tf.math.reduce_sum(vx ** 2)) * tf.math.sqrt(vary))\n",
    "    else:\n",
    "        correlation = prev_correlation\n",
    "    \n",
    "    mse = tf.math.reduce_mean((predictions - targets)**2)\n",
    "    \n",
    "    corr_weight = 10\n",
    "    mse_weight = 0.1\n",
    "    cost = corr_weight*(1 - correlation) + mse_weight*mse\n",
    "    \n",
    "    return cost, correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enformer\n",
    "import importlib\n",
    "importlib.reload(enformer)\n",
    "\n",
    "learning_rate = tf.Variable(0., trainable=False, name='learning_rate')\n",
    "initial_learning_rate = 0.00005\n",
    "decay_steps = 5000\n",
    "alpha = 1/20\n",
    "\n",
    "learning_rate.assign(initial_learning_rate)\n",
    "\n",
    "optimizer = snt.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "model = enformer.Enformer(channels=1536 // 4,\n",
    "                          num_heads=4,\n",
    "                          num_transformer_layers=5,\n",
    "                          pooling_type='max')\n",
    "\n",
    "train_step = create_step_function(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decayed_learning_rate(step):\n",
    "    step = min(step, decay_steps)\n",
    "    cosine_decay = 0.5 * (1 + math.cos(math.pi * step / decay_steps))\n",
    "    decayed = (1 - alpha) * cosine_decay + alpha\n",
    "    learning_rate.assign(initial_learning_rate * decayed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584351f",
   "metadata": {},
   "source": [
    "# Training/Evaluation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ad457",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 10000\n",
    "num_epochs = 50\n",
    "report_stats_every_n_steps = 5000\n",
    "n_batches = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed568254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_step_function(model, optimizer):\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(sequence, targets, embeddings):\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(sequence, embeddings, is_training=True)           \n",
    "            loss = criterion(targets, outputs)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        return loss, gradients, outputs\n",
    "    \n",
    "    return train_step\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, embeddings):\n",
    "    @tf.function\n",
    "    def predict(sequence, embeddings):\n",
    "        return model(sequence, embeddings, is_training=False)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    batch_losses = []\n",
    "    for samples_batch in dataset:\n",
    "        sequence = samples_batch._input_batches['sequence_batch']\n",
    "        \n",
    "        targets = samples_batch._target_batch\n",
    "        #tars = tf.expand_dims(targets, axis=1)\n",
    "        \n",
    "        output = predict(sequence, embeddings)\n",
    "        all_predictions.append(output.numpy().flatten())\n",
    "        all_targets.append(targets.numpy().flatten())\n",
    "\n",
    "#         loss = criterion(targets, output)\n",
    "        loss = correlation_criterion(targets, output, evaluate=True)\n",
    "        batch_losses.append(loss)\n",
    "    \n",
    "    prev_correlations_validate[:-300] = []\n",
    "    return np.average(batch_losses), np.array(all_predictions), np.array(all_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec9635c",
   "metadata": {},
   "source": [
    "# Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced2c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = 'debug'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd854abe",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'results/' + direc\n",
    "\n",
    "checkpoint_root = os.path.join(output_dir, \"checkpoints/\")\n",
    "checkpoint = tf.train.Checkpoint(module=model)\n",
    "checkpoint_name = \"best_model\"\n",
    "save_prefix = checkpoint_root\n",
    "manager = tf.train.CheckpointManager(checkpoint, save_prefix, checkpoint_name=checkpoint_name, max_to_keep=2)\n",
    "\n",
    "tensorboard_writer = SummaryWriter(os.path.join(output_dir))\n",
    "\n",
    "validation_data = create_validation_set(validation_sampler, n_samples = n_validation_samples)\n",
    "\n",
    "evaluate_model(model, [validation_data[0]], validation_embeddings) # initialize model variables\n",
    "\n",
    "min_loss = float(\"inf\")\n",
    "opt_step = 0\n",
    "\n",
    "train_losses = []\n",
    "train_preds = []\n",
    "train_targets = []\n",
    "\n",
    "\n",
    "# constant to scale sum of gradient\n",
    "const = tf.constant(1/n_batches)\n",
    "# get all trainable variables\n",
    "t_vars = model.trainable_variables\n",
    "# create a copy of all trainable variables with `0` as initial values\n",
    "accum_tvars = [tf.Variable(tf.zeros_like(t_var),trainable=False) for t_var in t_vars]   \n",
    "zero_ops = [tv.assign(tf.zeros_like(tv)) for tv in accum_tvars]\n",
    "\n",
    "num_steps = num_epochs * steps_per_epoch\n",
    "    \n",
    "for index in tqdm(range(num_steps)):\n",
    "    samples_batch, target_cells = sampler.sample()\n",
    "    \n",
    "    sequence = samples_batch._input_batches['sequence_batch']\n",
    "    \n",
    "    targets = samples_batch._target_batch\n",
    "    \n",
    "    loss, gradients, train_outputs = train_step(sequence, targets, train_embeddings)\n",
    "    train_losses.append(loss)\n",
    "    train_preds.append(train_outputs.numpy().flatten())\n",
    "    train_targets.append(targets.numpy().flatten())\n",
    "    \n",
    "    accum_ops = [accum_tvars[i].assign_add(tf.scalar_mul(const, grad)) for i, grad in enumerate(gradients)]\n",
    "\n",
    "    if (index + 1) % n_batches == 0:\n",
    "        optimizer.apply(accum_tvars, model.trainable_variables)\n",
    "        zero_ops = [tv.assign(tf.zeros_like(tv)) for tv in accum_tvars]\n",
    "\n",
    "        opt_step += 1        \n",
    "        # LEARNING RATE DECAY\n",
    "        decayed_learning_rate(opt_step)\n",
    "    \n",
    "    if index and index % report_stats_every_n_steps == 0:\n",
    "        validation_loss, validation_predictions, validation_targets = evaluate_model(model, validation_data, validation_embeddings)\n",
    "        if validation_loss < min_loss:\n",
    "            min_loss = validation_loss\n",
    "            save_checkpoint(manager, is_best=True)\n",
    "            print('Saving best loss', min_loss)\n",
    "            \n",
    "        train_loss = np.average(train_losses)\n",
    "        train_losses[:] = []\n",
    "        \n",
    "        train_metrics = calc_metrics(np.array(train_targets), np.array(train_preds))\n",
    "        train_targets[:] = []\n",
    "        train_preds[:] = []\n",
    "        \n",
    "        validation_metrics = calc_metrics(validation_targets, validation_predictions)\n",
    "            \n",
    "        step = index\n",
    "        log_metrics(tensorboard_writer, step, train_loss, validation_loss, train_metrics, validation_metrics)\n",
    "        prev_correlations_train[:-300] = []\n",
    "\n",
    "# CHECKPOINT FINAL VALUES?\n",
    "tensorboard_writer.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
