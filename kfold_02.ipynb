{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "# sys.path.append('../../')\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from omegaconf import OmegaConf\n",
    "import itertools\n",
    "from selene_sdk.utils import load_path, parse_configs_and_run\n",
    "from selene_sdk.utils.config_utils import module_from_dir, module_from_file\n",
    "from selene_sdk.utils.config import instantiate\n",
    "from src.dataset import EncodeDataset, LargeRandomSampler, encode_worker_init_fn\n",
    "from src.transforms import *\n",
    "from src.utils import interval_from_line\n",
    "# from torchvision import transforms\n",
    "# from torchmetrics import BinnedAveragePrecision, AveragePrecision, Accuracy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import copy\n",
    "from src.utils import expand_dims\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "from src.metrics import jaccard_score, threshold_wrapper\n",
    "from sklearn.metrics import average_precision_score\n",
    "from selene_sdk.utils.performance_metrics import compute_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "path = 'model_configs/biox_dnase_multi_ct_crossval.yaml'\n",
    "configs = load_path(path, instantiate=False)\n",
    "configs['dataset']['debug'] = True\n",
    "configs['dataset']['loader_args']['batch_size'] = 20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "from src.deepct_model_multi_ct import DeepCT\n",
    "\n",
    "model = DeepCT(**configs['model']['class_args'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from selene_sdk.utils.config_utils import get_full_dataset\n",
    "\n",
    "full_dataset = get_full_dataset(configs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEBUG MODE ON: 1000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "\n",
    "n_folds = 10\n",
    "k_fold = KFold(n_folds, shuffle=True, random_state=666)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dataset_info = configs[\"dataset\"]\n",
    "\n",
    "# all intervals\n",
    "genome_intervals = []\n",
    "with open(dataset_info[\"sampling_intervals_path\"])  as f:\n",
    "    for line in f:\n",
    "        chrom, start, end = interval_from_line(line)\n",
    "        genome_intervals.append((chrom, start, end))\n",
    "\n",
    "# bedug mode\n",
    "if dataset_info['debug']:\n",
    "    genome_intervals = random.sample(genome_intervals, k=1000)\n",
    "    print(\"DEBUG MODE ON:\", len(genome_intervals))\n",
    "\n",
    "with open(dataset_info[\"distinct_features_path\"]) as f:\n",
    "    distinct_features = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "\n",
    "# print(len(distinct_features))\n",
    "\n",
    "with open(dataset_info[\"target_features_path\"]) as f:\n",
    "    target_features = list(map(lambda x: x.rstrip(), f.readlines()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEBUG MODE ON: 1000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# genome_intervals"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "splits = []\n",
    "for train_idx, test_idx in k_fold.split(genome_intervals):\n",
    "    splits.append((train_idx, test_idx))\n",
    "\n",
    "len(splits)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "ct_masks = np.array_split(range(configs['model']['class_args']['n_cell_types']), n_folds)\n",
    "[len(c) for c in ct_masks]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[64, 63, 63, 63, 63, 63, 63, 63, 63, 63]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "train_folds_idx = splits[0][0]\n",
    "valid_folds_idx = splits[0][1]\n",
    "\n",
    "len(train_folds_idx), len(valid_folds_idx)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(900, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "current_fold_idx = np.append(train_folds_idx, valid_folds_idx)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "len(current_fold_idx)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def create_split_loaders(configs, full_dataset, split):\n",
    "    \"\"\"\n",
    "    Called for each split, this creates a two DataLoaders for each split. \n",
    "    One DataLoader for the samples in the training folds and one DataLoader \n",
    "    for the samples in the validation fold.\n",
    "    \"\"\"\n",
    "    dataset_info = configs[\"dataset\"]\n",
    "    # current_fold = configs[\"dataset\"]['dataset_args']['fold']\n",
    "    # print('current fold:', current_fold)\n",
    "\n",
    "    train_folds_idx = split[0]\n",
    "    valid_folds_idx = split[1]\n",
    "    current_fold_idx = np.append(train_folds_idx, valid_folds_idx)\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(\n",
    "        full_dataset, \n",
    "        current_fold_idx\n",
    "        )\n",
    "\n",
    "    val_subset = torch.utils.data.Subset(\n",
    "        full_dataset, \n",
    "        valid_folds_idx\n",
    "        )\n",
    "    val_transform = instantiate(dataset_info[\"val_transform\"])\n",
    "    val_subset.dataset.transform = val_transform\n",
    "\n",
    "    module = None\n",
    "    if os.path.isdir(dataset_info[\"path\"]):\n",
    "        module = module_from_dir(dataset_info[\"path\"])\n",
    "    else:\n",
    "        module = module_from_file(dataset_info[\"path\"])\n",
    "\n",
    "    train_sampler_class = getattr(module, dataset_info[\"sampler_class\"])\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(configs[\"random_seed\"])\n",
    "    train_sampler = train_sampler_class(\n",
    "        train_subset, replacement=False, generator=gen\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=dataset_info[\"loader_args\"][\"batch_size\"],\n",
    "        num_workers=dataset_info[\"loader_args\"][\"num_workers\"],\n",
    "        worker_init_fn=module.subset_encode_worker_init_fn,\n",
    "        sampler=train_sampler,\n",
    "    )\n",
    "\n",
    "    val_sampler_class = getattr(module, dataset_info[\"validation_sampler_class\"])\n",
    "    gen = torch.Generator()\n",
    "    gen.manual_seed(configs[\"random_seed\"])\n",
    "\n",
    "    val_sampler = val_sampler_class(\n",
    "        data_source=val_subset, \n",
    "        num_samples=dataset_info['validation_sampler_args']['num_samples'], \n",
    "        generator=gen\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "            val_subset,\n",
    "            batch_size=configs['dataset'][\"loader_args\"][\"batch_size\"],\n",
    "            num_workers=configs['dataset'][\"loader_args\"][\"num_workers\"],\n",
    "            worker_init_fn=module.subset_encode_worker_init_fn,\n",
    "            sampler=val_sampler,\n",
    "        )\n",
    "\n",
    "    return (train_loader, val_loader) \n",
    "\n",
    "\n",
    "def get_all_split_loaders(dataset, cv_splits):\n",
    "    \"\"\"Create DataLoaders for each split.\n",
    "\n",
    "    Keyword arguments:\n",
    "    dataset -- Dataset to sample from.\n",
    "    cv_splits -- Array containing indices of samples to \n",
    "                 be used in each fold for each split.\n",
    "    aug_count -- Number of variations for each sample in dataset.\n",
    "    batch_size -- batch size.\n",
    "    \n",
    "    \"\"\"\n",
    "    split_samplers = []\n",
    "    \n",
    "    for i in range(len(cv_splits)):\n",
    "        split_samplers.append(\n",
    "            create_split_loaders(\n",
    "                configs,\n",
    "                dataset,\n",
    "                cv_splits[i]\n",
    "                )\n",
    "        )\n",
    "    return split_samplers\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "train_loader_0, val_loader_0 = create_split_loaders(configs, full_dataset, splits[0])\n",
    "len(train_loader_0), len(val_loader_0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "dataloaders = get_all_split_loaders(full_dataset, splits)\n",
    "len(dataloaders)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "dataloaders[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fc79232feb8>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fc791e55a20>)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "for i, batch in tqdm(enumerate(dataloaders[0][1])):\n",
    "    sequence_batch, cell_type_batch, targets, target_mask = batch\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "target_mask.sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(12620)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "# torch.masked_select(target_mask, torch.tensor(ct_masks[0]))\n",
    "\n",
    "target_mask_tr = target_mask.clone()\n",
    "target_mask_tr[:, ct_masks[0].min(): ct_masks[0].max()+1] = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "target_mask_tr.sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(11340)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "def train(model, batch, fold):\n",
    "    \"\"\"\n",
    "    Trains the model on a batch of data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The training loss.\n",
    "\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # retrieved_seq, cell_type, target, target_mask\n",
    "    sequence_batch = batch[0]#.to(device)\n",
    "    cell_type_batch = batch[1]#.to(device)\n",
    "    targets = batch[2]#.to(device)\n",
    "    target_mask = batch[3]#.to(device)\n",
    "\n",
    "    # make train mask\n",
    "    target_mask_tr = target_mask.clone()\n",
    "    target_mask_tr[:, ct_masks[fold].min(): ct_masks[fold].max()+1] = False\n",
    "\n",
    "    outputs = model(sequence_batch, cell_type_batch)\n",
    "\n",
    "    criterion.weight = target_mask_tr\n",
    "    loss = criterion(outputs, targets)\n",
    "    if criterion.reduction == \"sum\":\n",
    "        loss = loss / criterion.weight.sum()\n",
    "    predictions = torch.sigmoid(outputs)\n",
    "\n",
    "    # predictions = predictions * target_mask_tr\n",
    "    # targets = targets * target_mask_tr\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return (\n",
    "        predictions.detach().numpy(),\n",
    "        targets.detach().numpy(),\n",
    "        target_mask_tr.numpy(),\n",
    "        loss.item(),\n",
    "    )\n",
    "\n",
    "def evaluate(model, batch, target_mask_tr):\n",
    "    \"\"\"\n",
    "    Makes predictions for some labeled input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_in_batches : list(SamplesBatch)\n",
    "        A list of tuples of the data, where the first element is\n",
    "        the example, and the second element is the label.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple(float, list(numpy.ndarray))\n",
    "        Returns the average loss, and the list of all predictions.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    batch_losses = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_target_masks = []\n",
    "\n",
    "    sequence_batch = batch[0]#.to(device)\n",
    "    cell_type_batch = batch[1]#.to(device)\n",
    "    targets = batch[2]#.to(device)\n",
    "    target_mask = batch[3]#.to(device)\n",
    "    # print('targets', targets.shape)\n",
    "\n",
    "    # val mask\n",
    "    target_mask_val = target_mask.clone()\n",
    "    target_mask_val = ~target_mask_tr\n",
    "\n",
    "    if target_mask_val.shape[0] != targets.shape[0]:\n",
    "        target_mask_val = target_mask_val[:targets.shape[0], ...]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sequence_batch, cell_type_batch)\n",
    "\n",
    "        criterion.weight = target_mask_val\n",
    "        loss = criterion(outputs, targets)\n",
    "        if criterion.reduction == \"sum\":\n",
    "            loss = loss / criterion.weight.sum()\n",
    "\n",
    "        predictions = torch.sigmoid(outputs)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        targets = targets.view(-1, targets.shape[-1])\n",
    "\n",
    "        target_mask = target_mask_val.view(-1, target_mask_val.shape[-1])\n",
    "\n",
    "        all_predictions.append(predictions.data.numpy())\n",
    "        all_targets.append(targets.data.numpy())\n",
    "        all_target_masks.append(target_mask.data.numpy())\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "    all_predictions = expand_dims(np.concatenate(all_predictions))\n",
    "    all_targets = expand_dims(np.concatenate(all_targets))\n",
    "    all_target_masks = expand_dims(np.concatenate(all_target_masks))\n",
    "\n",
    "    return np.average(batch_losses), all_predictions, all_targets, all_target_masks\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001, weight_decay = 1e-6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# train_batch_loader -- batches all samples in training folds.\n",
    "# valid_batch_loader -- batches all samples in validation fold.\n",
    "for fold, (train_batch_loader, valid_batch_loader) in enumerate(dataloaders):\n",
    "    # Loop through all batches in training folds for a given split.\n",
    "    model.train()\n",
    "    for batch in train_batch_loader:\n",
    "        # Train model on the training folds in the split.\n",
    "        prediction, target, target_mask, loss = train(model, batch, fold)\n",
    "\n",
    "        break\n",
    "    \n",
    "    # Loop through all batches in validation fold for a given split.\n",
    "    model.eval()\n",
    "    for batch in valid_batch_loader:\n",
    "        # Test model on the validation fold in the split.\n",
    "        (\n",
    "            average_loss,\n",
    "            all_predictions,\n",
    "            all_targets,\n",
    "            all_target_masks,\n",
    "        ) = evaluate(model, batch, target_mask_tr)\n",
    "        print(average_loss)\n",
    "        break\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6919394135475159\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "n_folds = 5\n",
    "ct_mask_range = np.array_split(range(configs['model']['class_args']['n_cell_types']), n_folds)\n",
    "mask_iterator = itertools.cycle(ct_mask_range)\n",
    "\n",
    "val_mask_idx = next(mask_iterator)\n",
    "val_mask_idx"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DeepCT(\n",
       "  (conv_net): Sequential(\n",
       "    (0): Conv1d(4, 320, kernel_size=(8,), stride=(1,))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv1d(320, 320, kernel_size=(8,), stride=(1,))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Conv1d(320, 480, kernel_size=(8,), stride=(1,))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv1d(480, 480, kernel_size=(8,), stride=(1,))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): BatchNorm1d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Dropout(p=0.2, inplace=False)\n",
       "    (13): Conv1d(480, 960, kernel_size=(8,), stride=(1,))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): Conv1d(960, 960, kernel_size=(8,), stride=(1,))\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (sequence_net): Sequential(\n",
       "    (0): Linear(in_features=42240, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (cell_type_net): Sequential(\n",
       "    (0): Linear(in_features=631, out_features=32, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=288, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (17): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001, weight_decay = 1e-6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def create_random_samples(t):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    ids = torch.multinomial(\n",
    "        input=torch.ones(t.shape[0]).flatten(),\n",
    "        num_samples=5,\n",
    "        replacement=False,\n",
    "    )\n",
    "    return ids "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "map_baseline_list = []\n",
    "map_model_list = []\n",
    "\n",
    "predictions_list = []\n",
    "baselines = []\n",
    "targets_list = []\n",
    "val_masks = []\n",
    "\n",
    "\n",
    "mask_iterator = itertools.cycle(ct_mask_range)\n",
    "\n",
    "for i, batch in tqdm(enumerate(full_dataloader)):\n",
    "    sequence_batch, cell_type_batch, targets, target_mask = batch\n",
    "\n",
    "    # new masks\n",
    "    val_mask_idx = next(mask_iterator)\n",
    "    target_mask_tr = target_mask.clone()\n",
    "    target_mask_tr[:, val_mask_idx[0]: val_mask_idx[-1]+1] = False\n",
    "    target_mask_val = ~target_mask_tr\n",
    "\n",
    "    # compute baseline (mean feature on train step)\n",
    "    mean_seq_tr = (targets * target_mask_tr).sum(axis=1) / target_mask_tr.sum(axis=1)\n",
    "    # print('mean_seq_tr', mean_seq_tr)\n",
    "    mean_seq_batch = torch.repeat_interleave(mean_seq_tr.unsqueeze(1), 631, dim=1)\n",
    "    # print(mean_seq_batch.shape)\n",
    "\n",
    "    # model\n",
    "    logits = model(sequence_batch, cell_type_batch)\n",
    "    predictions = torch.sigmoid(logits)\n",
    "\n",
    "    # для обучения используем target_mask_tr\n",
    "    criterion.weight = target_mask_tr\n",
    "    loss = criterion(predictions, targets.float())\n",
    "    loss = loss / criterion.weight.sum()\n",
    "    # print(\"train loss:\", loss.item())\n",
    "\n",
    "    # compute baseline score on val step\n",
    "    map_baseline, ap_baseline = compute_score(\n",
    "        mean_seq_batch.detach().numpy(), \n",
    "        targets.detach().numpy(), \n",
    "        average_precision_score, \n",
    "        target_mask=target_mask_val.detach().numpy(),\n",
    "        )\n",
    "    # compute model score on val step\n",
    "    map_model, ap_model = compute_score(\n",
    "        predictions.detach().numpy(), \n",
    "        targets.detach().numpy(), \n",
    "        average_precision_score, \n",
    "        target_mask=target_mask_val.detach().numpy(),\n",
    "        )\n",
    "\n",
    "    print(map_baseline, map_model)\n",
    "\n",
    "    map_baseline_list.append(map_baseline)\n",
    "    map_model_list.append(map_model)\n",
    "\n",
    "    # accum preds and targets\n",
    "    # ids = create_random_samples(targets)\n",
    "    # targets_list.append(targets[ids, :])\n",
    "    # predictions_list.append(predictions[ids, :])\n",
    "    # val_masks.append(target_mask_val[ids, :])\n",
    "    # baselines.append(mean_seq_batch[ids, :])\n",
    "\n",
    "    if i > 3:\n",
    "        break\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "1it [00:02,  2.21s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4791359413406657 0.07153907834551546\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2it [00:04,  2.42s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.2166265392455869 0.08058089365585772\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "3it [00:06,  2.23s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.09720248040736762 0.029146953564666213\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4it [00:09,  2.44s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.13918852726275693 0.03524491458875838\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4it [00:11,  2.92s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8111756077086069 0.2407245478068364\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "source": [
    "# на полном батче\n",
    "np.mean(map_baseline_list), np.mean(map_model_list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.3070952526525577, 0.05648780249548171)"
      ]
     },
     "metadata": {},
     "execution_count": 206
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "source": [
    "45220/3000"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15.073333333333334"
      ]
     },
     "metadata": {},
     "execution_count": 225
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "source": [
    "targets_cat = torch.cat(targets_list, dim=0)\n",
    "predictions_cat = torch.cat(predictions_list, dim=0)\n",
    "val_masks_cat = torch.cat(val_masks, dim=0)\n",
    "baselines_cat = torch.cat(baselines, dim=0)\n",
    "\n",
    "# compute metrics\n",
    "map_baseline, ap_baseline = compute_score(\n",
    "    baselines_cat.detach().numpy(), \n",
    "    targets_cat.detach().numpy(), \n",
    "    average_precision_score, \n",
    "    target_mask=val_masks_cat.detach().numpy(),\n",
    "    )\n",
    "\n",
    "map_model, ap_model = compute_score(\n",
    "    predictions_cat.detach().numpy(), \n",
    "    targets_cat.detach().numpy(), \n",
    "    average_precision_score, \n",
    "    target_mask=val_masks_cat.detach().numpy(),\n",
    "    )    \n",
    "\n",
    "map_baseline, map_model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.07229601151752332, 0.03363876850943481)"
      ]
     },
     "metadata": {},
     "execution_count": 208
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "source": [
    "val_masks_cat[0].sum(), val_masks_cat[1].sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor(127), tensor(127))"
      ]
     },
     "metadata": {},
     "execution_count": 212
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "indices = torch.multinomial(\n",
    "        input=torch.ones(targets.shape[0]).flatten(),\n",
    "        num_samples=5,\n",
    "        replacement=False,\n",
    "    )\n",
    "\n",
    "indices"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 1, 9, 3, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "targets[indices, :].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 631, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "# на случ подвыборке\n",
    "\n",
    "map_baseline_list = []\n",
    "map_model_list = []\n",
    "\n",
    "predictions_list = []\n",
    "baselines = []\n",
    "targets_list = []\n",
    "val_masks = []\n",
    "\n",
    "mask_iterator = itertools.cycle(ct_mask_range)\n",
    "\n",
    "for i, batch in tqdm(enumerate(full_dataloader)):\n",
    "    sequence_batch, cell_type_batch, targets, target_mask = batch\n",
    "\n",
    "    # new masks\n",
    "    val_mask_idx = next(mask_iterator)\n",
    "    target_mask_tr = target_mask.clone()\n",
    "    target_mask_tr[:, val_mask_idx[0]: val_mask_idx[-1]+1] = False\n",
    "    target_mask_val = ~target_mask_tr\n",
    "\n",
    "    # compute baseline\n",
    "    mean_seq_tr = (targets * target_mask_tr).sum(axis=1) / target_mask_tr.sum(axis=1)\n",
    "    # print('mean_seq_tr', mean_seq_tr)\n",
    "    mean_seq_batch = torch.repeat_interleave(mean_seq_tr.unsqueeze(1), 631, dim=1)\n",
    "    # print(mean_seq_batch.shape)\n",
    "\n",
    "    # model\n",
    "    logits = model(sequence_batch, cell_type_batch)\n",
    "    predictions = torch.sigmoid(logits)\n",
    "\n",
    "    criterion.weight = target_mask_tr\n",
    "    loss = criterion(predictions, targets.float())\n",
    "    loss = loss / criterion.weight.sum()\n",
    "    # print(\"train loss:\", loss.item())\n",
    "\n",
    "    # compute metrics\n",
    "    map_baseline, ap_baseline = compute_score(\n",
    "        mean_seq_batch.detach().numpy(), \n",
    "        targets.detach().numpy(), \n",
    "        average_precision_score, \n",
    "        target_mask=target_mask_val.detach().numpy(),\n",
    "        )\n",
    "\n",
    "    map_model, ap_model = compute_score(\n",
    "        predictions.detach().numpy(), \n",
    "        targets.detach().numpy(), \n",
    "        average_precision_score, \n",
    "        target_mask=target_mask_val.detach().numpy(),\n",
    "        )    \n",
    "\n",
    "    print(map_baseline, map_model)\n",
    "    \n",
    "    map_baseline_list.append(map_baseline)\n",
    "    map_model_list.append(map_model)\n",
    "\n",
    "    # accum preds and targets\n",
    "    ids = create_random_samples(targets)\n",
    "    targets_list.append(targets[ids, :])\n",
    "    predictions_list.append(predictions[ids, :])\n",
    "    val_masks.append(target_mask_val[ids, :])\n",
    "    baselines.append(mean_seq_batch[ids, :])\n",
    "\n",
    "    if i > 3:\n",
    "        break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fe7eda426a0>>Exception ignored in: \n",
      "<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fe7eda426a0>>Traceback (most recent call last):\n",
      "  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "\n",
      "Traceback (most recent call last):\n",
      "      File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()self._shutdown_workers()\n",
      "\n",
      "  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
      "    if w.is_alive():    if w.is_alive():\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 134, in is_alive\n",
      "\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError\n",
      ": AssertionError: can only test a child process\n",
      "can only test a child process\n",
      "1it [00:03,  3.32s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.03430623803603497 0.02881303427174626\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2it [00:06,  3.53s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None None\n",
      "0.433577806122449 0.05816887400500888\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4it [00:13,  3.50s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.008272707231040566 0.008317516752354277\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "4it [00:17,  4.36s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.07802232003912675 0.03296399140914681\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "# на полном батче\n",
    "np.mean(map_baseline_list), np.mean(map_model_list)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'NoneType'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-ae8eab4bea92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# на полном батче\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_baseline_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_model_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/genv/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3373\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/genv/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'NoneType'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.14884606016357732, 0.02611064614409038)"
      ]
     },
     "metadata": {},
     "execution_count": 186
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "map_model_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[None,\n",
       " 0.01994768862137945,\n",
       " 0.034396911356918525,\n",
       " 0.02605354163114649,\n",
       " 0.04695420318794074]"
      ]
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('genv': venv)"
  },
  "interpreter": {
   "hash": "7bed6a5b8f0827f7d7f30ba9d7b2c61fcd2f223009d9afc95ff59e669a7c4ade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}