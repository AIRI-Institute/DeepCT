{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "# sys.path.append('../../')\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from omegaconf import OmegaConf\n",
    "import itertools\n",
    "from selene_sdk.utils import load_path, parse_configs_and_run\n",
    "from selene_sdk.utils.config_utils import module_from_dir, module_from_file\n",
    "from selene_sdk.utils.config import instantiate\n",
    "from src.dataset import EncodeDataset, LargeRandomSampler, SubsetRandomSampler, encode_worker_init_fn\n",
    "from src.transforms import *\n",
    "from src.utils import interval_from_line\n",
    "# from torchvision import transforms\n",
    "# from torchmetrics import BinnedAveragePrecision, AveragePrecision, Accuracy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import copy\n",
    "from src.utils import expand_dims\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "from src.metrics import jaccard_score, threshold_wrapper\n",
    "from sklearn.metrics import average_precision_score\n",
    "from selene_sdk.utils.performance_metrics import compute_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "ct_means = np.load('results/ct_mean_targets_02.npy', allow_pickle=True)\n",
    "ct_means.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(631,)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "path = 'model_configs/biox_dnase_multi_ct_crossval_shuffle_loaders_fold_00.yaml'\n",
    "configs = load_path(path, instantiate=False)\n",
    "# configs['dataset']['path'] = 'src/bad_dataset.py'\n",
    "configs['dataset']['debug'] = True\n",
    "configs['dataset']['loader_args']['batch_size'] = 20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# from src.deepct_model_multi_ct import DeepCT\n",
    "\n",
    "# model = DeepCT(**configs['model']['class_args'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from selene_sdk.utils.config_utils import get_full_dataset, get_full_dataloader\n",
    "\n",
    "full_dataset = get_full_dataset(configs)\n",
    "# full_dataloader = get_full_dataloader(configs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEBUG MODE ON: 1000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "dataset_info = configs[\"dataset\"]\n",
    "\n",
    "# all intervals\n",
    "genome_intervals = []\n",
    "with open(dataset_info[\"sampling_intervals_path\"])  as f:\n",
    "    for line in f:\n",
    "        chrom, start, end = interval_from_line(line)\n",
    "        if chrom not in dataset_info[\"test_holdout\"]:\n",
    "            genome_intervals.append((chrom, start, end))\n",
    "\n",
    "# bedug mode\n",
    "if dataset_info['debug']:\n",
    "    genome_intervals = random.sample(genome_intervals, k=1000)\n",
    "    print(\"DEBUG MODE ON:\", len(genome_intervals))\n",
    "\n",
    "print(len(genome_intervals))  # 1248877 vs 1377454\n",
    "\n",
    "with open(dataset_info[\"distinct_features_path\"]) as f:\n",
    "    distinct_features = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "\n",
    "with open(dataset_info[\"target_features_path\"]) as f:\n",
    "    target_features = list(map(lambda x: x.rstrip(), f.readlines()))\n",
    "\n",
    "\n",
    "random.seed(666)\n",
    "\n",
    "genome_intervals_arr = np.asarray(genome_intervals, dtype='U10,i8,i8')\n",
    "random.shuffle(genome_intervals_arr)\n",
    "seq_splits = np.array_split(genome_intervals_arr, 10)\n",
    "len(seq_splits)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEBUG MODE ON: 1000\n",
      "1000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# kfold_intervals = []\n",
    "# for train_intervals in seq_splits:\n",
    "#     val_size = int(len(train_intervals)*0.2)\n",
    "#     random.seed(666)\n",
    "#     val_intervals = random.sample(train_intervals.tolist(), val_size)\n",
    "#     kfold_intervals.append((train_intervals, val_intervals))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# kfold_intervals[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "ct_list = list(range(configs['model']['class_args']['n_cell_types'])) \n",
    "ct_masks = []\n",
    "for fold in range(10):\n",
    "    random.seed(666)\n",
    "    random.shuffle(ct_list)\n",
    "    ct_masks.append(np.array_split(ct_list, 10))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "len(ct_masks[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "sum([len(c) for c in ct_masks[0]])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "631"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "len(ct_masks[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "ct_masks = np.load(configs['dataset']['ct_fold_ids'], allow_pickle=True)\n",
    "# набор масок для текущей модели\n",
    "curr_fold = configs['dataset']['dataset_args']['fold']\n",
    "ct_masks = ct_masks[curr_fold]\n",
    "print('# cell_type folds:', len(ct_masks))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# cell_type folds: 10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "[len(c) for c in ct_masks]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[64, 63, 63, 63, 63, 63, 63, 63, 63, 63]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from selene_sdk.utils.config_utils import get_all_split_loaders\n",
    "\n",
    "\n",
    "splits = np.load(configs['dataset']['seq_fold_ids'], allow_pickle=True)\n",
    "full_dataset = get_full_dataset(configs)\n",
    "print('full_dataset len:', len(full_dataset))\n",
    "dataloaders = get_all_split_loaders(configs, full_dataset, splits)\n",
    "print('# dataloaders:', len(dataloaders))\n",
    "print('train/val lens:', len(dataloaders[0][0]), len(dataloaders[0][1]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEBUG MODE ON: 1000\n",
      "full_dataset len: 7885\n",
      "# dataloaders: 10\n",
      "train/val lens: 65366 13324\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "dataloaders[0][0].dataset.transform, dataloaders[0][1].dataset.transform"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Compose(\n",
       "     PermuteSequenceChannels()\n",
       "     RandomReverseStrand()\n",
       " ),\n",
       " PermuteSequenceChannels())"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "dataloaders"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(<torch.utils.data.dataloader.DataLoader at 0x7f76c3673710>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7f7758359438>),\n",
       " (<torch.utils.data.dataloader.DataLoader at 0x7f76b841fd68>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7f76b73b6630>),\n",
       " (<torch.utils.data.dataloader.DataLoader at 0x7f76c05324a8>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7f76bc29ccc0>),\n",
       " (<torch.utils.data.dataloader.DataLoader at 0x7f76bf9cbb38>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7f76ba9d3400>),\n",
       " (<torch.utils.data.dataloader.DataLoader at 0x7f76bfc4d240>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7f76b80ffb00>),\n",
       " (<torch.utils.data.dataloader.DataLoader at 0x7f76bdc55978>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7f76ba447278>),\n",
       " (<torch.utils.data.dataloader.DataLoader at 0x7f76bb3aa0b8>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7f76b8097940>),\n",
       " (<torch.utils.data.dataloader.DataLoader at 0x7f76c36c2780>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7f76b711e080>),\n",
       " (<torch.utils.data.dataloader.DataLoader at 0x7f76bc3b2e80>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7f76c21a87b8>),\n",
       " (<torch.utils.data.dataloader.DataLoader at 0x7f76b9ab3630>,\n",
       "  <torch.utils.data.dataloader.DataLoader at 0x7f76c0e81eb8>)]"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "fold_map = {}\n",
    "\n",
    "for i in range(len(dataloaders)):\n",
    "    fold_map[dataloaders[i]] = ct_masks[i]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "len(fold_map.keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "random.seed(666)\n",
    "l = list(fold_map.items())\n",
    "for chunk, ((train_batch_loader, valid_batch_loader), current_ct_mask) in enumerate(dict(random.sample(l, len(fold_map.keys()))).items()):\n",
    "    print(chunk, current_ct_mask[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 [  8 583 216 575 465]\n",
      "1 [387 623 587 278 480]\n",
      "2 [106 315 630 515 310]\n",
      "3 [307 229  74 591  55]\n",
      "4 [423 202 442 435 403]\n",
      "5 [ 40  37 139 298 206]\n",
      "6 [ 84 246 397 574 260]\n",
      "7 [ 60 239 485 449  80]\n",
      "8 [168 238 402 497 407]\n",
      "9 [195 143 588 100 288]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "random.seed(666)\n",
    "l = list(fold_map.items())\n",
    "for chunk, ((train_batch_loader, valid_batch_loader), current_ct_mask) in enumerate(dict(random.sample(l, 10)).items()):\n",
    "    if chunk >= 7:\n",
    "        print(chunk, current_ct_mask[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7 [ 60 239 485 449  80]\n",
      "8 [168 238 402 497 407]\n",
      "9 [195 143 588 100 288]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import math\n",
    "\n",
    "((1/4) * (2/3)**3)/((1/4) * (2/3)**3 + (3/4) * (1/3)**3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7272727272727272"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "# main train/val loop\n",
    "# random.seed(666)\n",
    "for epoch in tqdm(range(0, 10)):\n",
    "    print('epoch:', epoch)\n",
    "    # shuffle loaders\n",
    "    # random.seed(666)\n",
    "\n",
    "    random.seed(666+epoch)\n",
    "    l = list(fold_map.items())\n",
    "    for chunk, ((train_batch_loader, valid_batch_loader), current_ct_mask) in enumerate(dict(random.sample(l, len(fold_map.keys()))).items()):\n",
    "        print(chunk, current_ct_mask[:5])\n",
    "        \n",
    "    print('--------------')\n",
    "    # if epoch == 2:\n",
    "    #     break\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 318.06it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 0\n",
      "0 [  8 583 216 575 465]\n",
      "1 [387 623 587 278 480]\n",
      "2 [106 315 630 515 310]\n",
      "3 [307 229  74 591  55]\n",
      "4 [423 202 442 435 403]\n",
      "5 [ 40  37 139 298 206]\n",
      "6 [ 84 246 397 574 260]\n",
      "7 [ 60 239 485 449  80]\n",
      "8 [168 238 402 497 407]\n",
      "9 [195 143 588 100 288]\n",
      "--------------\n",
      "epoch: 1\n",
      "0 [195 143 588 100 288]\n",
      "1 [106 315 630 515 310]\n",
      "2 [387 623 587 278 480]\n",
      "3 [ 60 239 485 449  80]\n",
      "4 [168 238 402 497 407]\n",
      "5 [307 229  74 591  55]\n",
      "6 [423 202 442 435 403]\n",
      "7 [ 84 246 397 574 260]\n",
      "8 [ 40  37 139 298 206]\n",
      "9 [  8 583 216 575 465]\n",
      "--------------\n",
      "epoch: 2\n",
      "0 [  8 583 216 575 465]\n",
      "1 [195 143 588 100 288]\n",
      "2 [ 40  37 139 298 206]\n",
      "3 [168 238 402 497 407]\n",
      "4 [106 315 630 515 310]\n",
      "5 [ 84 246 397 574 260]\n",
      "6 [ 60 239 485 449  80]\n",
      "7 [307 229  74 591  55]\n",
      "8 [423 202 442 435 403]\n",
      "9 [387 623 587 278 480]\n",
      "--------------\n",
      "epoch: 3\n",
      "0 [423 202 442 435 403]\n",
      "1 [106 315 630 515 310]\n",
      "2 [  8 583 216 575 465]\n",
      "3 [ 60 239 485 449  80]\n",
      "4 [195 143 588 100 288]\n",
      "5 [307 229  74 591  55]\n",
      "6 [ 84 246 397 574 260]\n",
      "7 [387 623 587 278 480]\n",
      "8 [ 40  37 139 298 206]\n",
      "9 [168 238 402 497 407]\n",
      "--------------\n",
      "epoch: 4\n",
      "0 [ 84 246 397 574 260]\n",
      "1 [ 60 239 485 449  80]\n",
      "2 [168 238 402 497 407]\n",
      "3 [  8 583 216 575 465]\n",
      "4 [195 143 588 100 288]\n",
      "5 [387 623 587 278 480]\n",
      "6 [423 202 442 435 403]\n",
      "7 [106 315 630 515 310]\n",
      "8 [307 229  74 591  55]\n",
      "9 [ 40  37 139 298 206]\n",
      "--------------\n",
      "epoch: 5\n",
      "0 [ 84 246 397 574 260]\n",
      "1 [307 229  74 591  55]\n",
      "2 [387 623 587 278 480]\n",
      "3 [ 60 239 485 449  80]\n",
      "4 [168 238 402 497 407]\n",
      "5 [  8 583 216 575 465]\n",
      "6 [106 315 630 515 310]\n",
      "7 [ 40  37 139 298 206]\n",
      "8 [195 143 588 100 288]\n",
      "9 [423 202 442 435 403]\n",
      "--------------\n",
      "epoch: 6\n",
      "0 [ 40  37 139 298 206]\n",
      "1 [195 143 588 100 288]\n",
      "2 [168 238 402 497 407]\n",
      "3 [ 60 239 485 449  80]\n",
      "4 [106 315 630 515 310]\n",
      "5 [423 202 442 435 403]\n",
      "6 [307 229  74 591  55]\n",
      "7 [ 84 246 397 574 260]\n",
      "8 [  8 583 216 575 465]\n",
      "9 [387 623 587 278 480]\n",
      "--------------\n",
      "epoch: 7\n",
      "0 [106 315 630 515 310]\n",
      "1 [ 60 239 485 449  80]\n",
      "2 [  8 583 216 575 465]\n",
      "3 [168 238 402 497 407]\n",
      "4 [195 143 588 100 288]\n",
      "5 [ 40  37 139 298 206]\n",
      "6 [307 229  74 591  55]\n",
      "7 [ 84 246 397 574 260]\n",
      "8 [387 623 587 278 480]\n",
      "9 [423 202 442 435 403]\n",
      "--------------\n",
      "epoch: 8\n",
      "0 [387 623 587 278 480]\n",
      "1 [195 143 588 100 288]\n",
      "2 [106 315 630 515 310]\n",
      "3 [  8 583 216 575 465]\n",
      "4 [ 60 239 485 449  80]\n",
      "5 [ 84 246 397 574 260]\n",
      "6 [307 229  74 591  55]\n",
      "7 [423 202 442 435 403]\n",
      "8 [ 40  37 139 298 206]\n",
      "9 [168 238 402 497 407]\n",
      "--------------\n",
      "epoch: 9\n",
      "0 [168 238 402 497 407]\n",
      "1 [ 60 239 485 449  80]\n",
      "2 [387 623 587 278 480]\n",
      "3 [  8 583 216 575 465]\n",
      "4 [307 229  74 591  55]\n",
      "5 [ 84 246 397 574 260]\n",
      "6 [106 315 630 515 310]\n",
      "7 [195 143 588 100 288]\n",
      "8 [423 202 442 435 403]\n",
      "9 [ 40  37 139 298 206]\n",
      "--------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# main train/val loop\n",
    "for epoch in tqdm(range(1, 10)):\n",
    "    print('epoch:', epoch)\n",
    "    # shuffle loaders\n",
    "    random.seed(666)\n",
    "    l = list(fold_map.items())\n",
    "    random.shuffle(l)\n",
    "    fold_map = dict(l)\n",
    "\n",
    "    for chunk, ((train_batch_loader, valid_batch_loader), current_ct_mask) in enumerate(fold_map.items()):\n",
    "\n",
    "        print(current_ct_mask[:5])\n",
    "    print('--------------')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 421.09it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 1\n",
      "[168 238 402 497 407]\n",
      "[195 143 588 100 288]\n",
      "[ 40  37 139 298 206]\n",
      "[106 315 630 515 310]\n",
      "[307 229  74 591  55]\n",
      "[ 84 246 397 574 260]\n",
      "[423 202 442 435 403]\n",
      "[387 623 587 278 480]\n",
      "[ 60 239 485 449  80]\n",
      "[  8 583 216 575 465]\n",
      "--------------\n",
      "epoch: 2\n",
      "[195 143 588 100 288]\n",
      "[106 315 630 515 310]\n",
      "[ 84 246 397 574 260]\n",
      "[  8 583 216 575 465]\n",
      "[168 238 402 497 407]\n",
      "[307 229  74 591  55]\n",
      "[ 40  37 139 298 206]\n",
      "[ 60 239 485 449  80]\n",
      "[423 202 442 435 403]\n",
      "[387 623 587 278 480]\n",
      "--------------\n",
      "epoch: 3\n",
      "[106 315 630 515 310]\n",
      "[  8 583 216 575 465]\n",
      "[307 229  74 591  55]\n",
      "[387 623 587 278 480]\n",
      "[195 143 588 100 288]\n",
      "[168 238 402 497 407]\n",
      "[ 84 246 397 574 260]\n",
      "[423 202 442 435 403]\n",
      "[ 40  37 139 298 206]\n",
      "[ 60 239 485 449  80]\n",
      "--------------\n",
      "epoch: 4\n",
      "[  8 583 216 575 465]\n",
      "[387 623 587 278 480]\n",
      "[168 238 402 497 407]\n",
      "[ 60 239 485 449  80]\n",
      "[106 315 630 515 310]\n",
      "[195 143 588 100 288]\n",
      "[307 229  74 591  55]\n",
      "[ 40  37 139 298 206]\n",
      "[ 84 246 397 574 260]\n",
      "[423 202 442 435 403]\n",
      "--------------\n",
      "epoch: 5\n",
      "[387 623 587 278 480]\n",
      "[ 60 239 485 449  80]\n",
      "[195 143 588 100 288]\n",
      "[423 202 442 435 403]\n",
      "[  8 583 216 575 465]\n",
      "[106 315 630 515 310]\n",
      "[168 238 402 497 407]\n",
      "[ 84 246 397 574 260]\n",
      "[307 229  74 591  55]\n",
      "[ 40  37 139 298 206]\n",
      "--------------\n",
      "epoch: 6\n",
      "[ 60 239 485 449  80]\n",
      "[423 202 442 435 403]\n",
      "[106 315 630 515 310]\n",
      "[ 40  37 139 298 206]\n",
      "[387 623 587 278 480]\n",
      "[  8 583 216 575 465]\n",
      "[195 143 588 100 288]\n",
      "[307 229  74 591  55]\n",
      "[168 238 402 497 407]\n",
      "[ 84 246 397 574 260]\n",
      "--------------\n",
      "epoch: 7\n",
      "[423 202 442 435 403]\n",
      "[ 40  37 139 298 206]\n",
      "[  8 583 216 575 465]\n",
      "[ 84 246 397 574 260]\n",
      "[ 60 239 485 449  80]\n",
      "[387 623 587 278 480]\n",
      "[106 315 630 515 310]\n",
      "[168 238 402 497 407]\n",
      "[195 143 588 100 288]\n",
      "[307 229  74 591  55]\n",
      "--------------\n",
      "epoch: 8\n",
      "[ 40  37 139 298 206]\n",
      "[ 84 246 397 574 260]\n",
      "[387 623 587 278 480]\n",
      "[307 229  74 591  55]\n",
      "[423 202 442 435 403]\n",
      "[ 60 239 485 449  80]\n",
      "[  8 583 216 575 465]\n",
      "[195 143 588 100 288]\n",
      "[106 315 630 515 310]\n",
      "[168 238 402 497 407]\n",
      "--------------\n",
      "epoch: 9\n",
      "[ 84 246 397 574 260]\n",
      "[307 229  74 591  55]\n",
      "[ 60 239 485 449  80]\n",
      "[168 238 402 497 407]\n",
      "[ 40  37 139 298 206]\n",
      "[423 202 442 435 403]\n",
      "[387 623 587 278 480]\n",
      "[106 315 630 515 310]\n",
      "[  8 583 216 575 465]\n",
      "[195 143 588 100 288]\n",
      "--------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "for (train_batch_loader, valid_batch_loader), ct_mask in fold_map.items():\n",
    "    print(len(train_batch_loader), len(valid_batch_loader), len(ct_mask))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "55055 11050 63\n",
      "50855 10269 63\n",
      "50502 10047 63\n",
      "52922 10516 63\n",
      "65366 13324 64\n",
      "50500 10030 63\n",
      "53064 10540 63\n",
      "51413 10316 63\n",
      "51438 10423 63\n",
      "51486 10332 63\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "ct_mask"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([307, 229,  74, 591,  55,  13, 378, 134, 127, 176, 492, 562, 600,\n",
       "       599, 153, 446, 363, 528, 616, 162, 113, 251, 326, 245, 450, 137,\n",
       "         5, 241, 490,  65,  54, 595, 149, 205,  70, 286,  90, 496, 475,\n",
       "       453, 265,  33, 208, 211, 356, 214, 316, 366, 474, 438, 498, 484,\n",
       "       421, 531, 441, 148, 339, 170, 361, 384, 370, 199, 417])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for batch in valid_batch_loader:\n",
    "    sequence_batch = batch[0]\n",
    "    cell_type_batch = batch[1]\n",
    "    targets = batch[2]\n",
    "    target_mask = batch[3]\n",
    "\n",
    "    # val mask\n",
    "    target_mask_tr = target_mask.clone()\n",
    "    # !!!\n",
    "    target_mask_tr[:, ct_mask, :] = False\n",
    "    target_mask_val = ~target_mask_tr\n",
    "    break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "target_mask.sum(), target_mask_tr.sum(), target_mask_val.sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor(12620), tensor(11360), tensor(1260))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "target_mask_tr.sum() + target_mask_val.sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(12620)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([('chr10', 66184220, 66184455), ('chr6',   934320,   934680),\n",
       "       ('chr10', 66184220, 66184455)],\n",
       "      dtype=[('f0', '<U10'), ('f1', '<i8'), ('f2', '<i8')])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "seq_splits[0].shape[0]*0.25"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "31222.0"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "module = None\n",
    "if os.path.isdir(dataset_info[\"path\"]):\n",
    "    module = module_from_dir(dataset_info[\"path\"])\n",
    "else:\n",
    "    module = module_from_file(dataset_info[\"path\"])\n",
    "\n",
    "dataset_class = getattr(module, dataset_info[\"class\"])\n",
    "dataset_info[\"dataset_args\"][\"target_features\"] = target_features\n",
    "dataset_info[\"dataset_args\"][\"distinct_features\"] = distinct_features\n",
    "\n",
    "# load train dataset and loader\n",
    "data_config = dataset_info[\"dataset_args\"].copy()\n",
    "data_config[\"intervals\"] = seq_splits[0]#.tolist()\n",
    "\n",
    "del data_config['fold']\n",
    "del data_config['n_folds']\n",
    "train_subset = dataset_class(**data_config)\n",
    "\n",
    "train_sampler_class = getattr(module, dataset_info[\"sampler_class\"])\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(configs[\"random_seed\"])\n",
    "train_sampler = train_sampler_class(\n",
    "    train_subset, replacement=False, generator=gen\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=dataset_info[\"loader_args\"][\"batch_size\"],\n",
    "        num_workers=dataset_info[\"loader_args\"][\"num_workers\"],\n",
    "        worker_init_fn=module.encode_worker_init_fn,\n",
    "        sampler=train_sampler,\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "val_sampler_class = getattr(module, dataset_info[\"validation_sampler_class\"])\n",
    "gen = torch.Generator()\n",
    "gen.manual_seed(configs[\"random_seed\"])\n",
    "\n",
    "val_sampler = val_sampler_class(\n",
    "    data_source=train_subset, \n",
    "    num_samples=dataset_info['validation_sampler_args']['num_samples'], \n",
    "    generator=gen\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=configs['dataset'][\"loader_args\"][\"batch_size\"],\n",
    "        num_workers=configs['dataset'][\"loader_args\"][\"num_workers\"],\n",
    "        worker_init_fn=module.subset_encode_worker_init_fn,\n",
    "        sampler=val_sampler,\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "splits = []\n",
    "for train_idx, test_idx in k_fold.split(genome_intervals):\n",
    "    splits.append((train_idx, test_idx))\n",
    "\n",
    "len(splits)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "[len(s) for s in splits]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "len(splits[0][0]), len(splits[0][1])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(900, 100)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "np.save(f'/home/thurs/DeepCT/results/kfold_splits_hold.npy', splits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "splits = np.load(f'/home/thurs/DeepCT/results/kfold_splits_hold.npy', allow_pickle=True)\n",
    "len(splits)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "for i in splits:\n",
    "    print(i[0], i[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 124888  124889  124890 ... 1248874 1248875 1248876] [     0      1      2 ... 124885 124886 124887]\n",
      "[      0       1       2 ... 1248874 1248875 1248876] [124888 124889 124890 ... 249773 249774 249775]\n",
      "[      0       1       2 ... 1248874 1248875 1248876] [249776 249777 249778 ... 374661 374662 374663]\n",
      "[      0       1       2 ... 1248874 1248875 1248876] [374664 374665 374666 ... 499549 499550 499551]\n",
      "[      0       1       2 ... 1248874 1248875 1248876] [499552 499553 499554 ... 624437 624438 624439]\n",
      "[      0       1       2 ... 1248874 1248875 1248876] [624440 624441 624442 ... 749325 749326 749327]\n",
      "[      0       1       2 ... 1248874 1248875 1248876] [749328 749329 749330 ... 874213 874214 874215]\n",
      "[      0       1       2 ... 1248874 1248875 1248876] [874216 874217 874218 ... 999100 999101 999102]\n",
      "[      0       1       2 ... 1248874 1248875 1248876] [ 999103  999104  999105 ... 1123987 1123988 1123989]\n",
      "[      0       1       2 ... 1123987 1123988 1123989] [1123990 1123991 1123992 ... 1248874 1248875 1248876]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "splits[1][0], splits[1][1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([      0,       1,       2, ..., 1248873, 1248875, 1248876]),\n",
       " array([     10,      40,      50, ..., 1248846, 1248851, 1248874]))"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "random.seed(666)\n",
    "\n",
    "train_folds_idx = splits[0][0]\n",
    "valid_folds_idx = splits[0][1]\n",
    "current_fold_idx = np.append(train_folds_idx, valid_folds_idx)\n",
    "random.shuffle(current_fold_idx)\n",
    "current_fold_idx\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1140471,  732277,  782472, ..., 1012061,  878426, 1062745])"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(666)\n",
    "\n",
    "n_folds = configs['dataset']['dataset_args']['n_folds']\n",
    "ct_list = list(range(configs['model']['class_args']['n_cell_types'])) \n",
    "ct_masks = []\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    random.shuffle(ct_list)\n",
    "    # print(ct_list)\n",
    "    ct_masks.append(np.array_split(ct_list, n_folds))\n",
    "\n",
    "print([len(c) for c in ct_masks])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "ct_masks[0][0], ct_masks[0][1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 60, 239, 485, 449,  80, 455, 138,  85, 183, 328, 257,  16,  71,\n",
       "        32,   3, 594, 332, 401, 185, 473, 207, 413,  98, 493, 231, 306,\n",
       "       454,  66, 524, 470, 556, 610, 275, 519, 367, 536, 261,  68, 412,\n",
       "       428, 281, 406, 461, 572, 130, 547, 123,  67, 415, 424, 171, 615,\n",
       "       254, 608, 248, 567, 146,  47, 486, 272, 182, 362, 252, 522])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# 10 val masks\n",
    "[c.shape[0] for c in ct_masks[0]]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[64, 63, 63, 63, 63, 63, 63, 63, 63, 63]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "ct_masks[0][0].min(), ct_masks[0][0].max()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3, 615)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "np.save('results/ct_random_ids_k10.npy', ct_masks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "ct_masks = np.load('results/ct_random_ids_k10.npy', allow_pickle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "dataloaders = get_all_split_loaders(configs, full_dataset, kfold_intervals)\n",
    "len(dataloaders)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "dataloaders[0][0].dataset.dataset.transform, dataloaders[0][1].dataset.dataset.transform"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(PermuteSequenceChannels(), PermuteSequenceChannels())"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "train_loader_0, val_loader_0 = create_split_loaders(configs, full_dataset, splits[0])\n",
    "len(train_loader_0), len(val_loader_0)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(50, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "dataloaders = get_all_split_loaders(full_dataset, splits)\n",
    "len(dataloaders)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "for i, batch in tqdm(enumerate(dataloaders[0][1])):\n",
    "    sequence_batch, cell_type_batch, targets, target_mask = batch\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/dataset.py\", line 311, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"src/dataset.py\", line 219, in __getitem__\n    chrom, pos, cell_type_idx = self._get_chrom_pos_cell_by_idx(idx)\n  File \"src/dataset.py\", line 251, in _get_chrom_pos_cell_by_idx\n    interval_idx = bisect.bisect(self.intervals_length_sums, position_idx) - 1\nTypeError: '<' not supported between instances of 'tuple' and 'int'\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-d6ef83fb9985>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msequence_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_type_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/genv/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/genv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/genv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/genv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/genv/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/thurs/genv/lib/python3.6/site-packages/torch/utils/data/dataset.py\", line 311, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"src/dataset.py\", line 219, in __getitem__\n    chrom, pos, cell_type_idx = self._get_chrom_pos_cell_by_idx(idx)\n  File \"src/dataset.py\", line 251, in _get_chrom_pos_cell_by_idx\n    interval_idx = bisect.bisect(self.intervals_length_sums, position_idx) - 1\nTypeError: '<' not supported between instances of 'tuple' and 'int'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "targets.sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(134.)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "targets.size()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20, 631, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def train(model, batch, fold):\n",
    "    \"\"\"\n",
    "    Trains the model on a batch of data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The training loss.\n",
    "\n",
    "    \"\"\"    \n",
    "    # retrieved_seq, cell_type, target, target_mask\n",
    "    sequence_batch = batch[0]#.to(device)\n",
    "    cell_type_batch = batch[1]#.to(device)\n",
    "    targets = batch[2]#.to(device)\n",
    "    target_mask = batch[3]#.to(device)\n",
    "\n",
    "    # make train mask\n",
    "    target_mask_tr = target_mask.clone()\n",
    "    target_mask_tr[:, ct_masks[fold].min(): ct_masks[fold].max()+1] = False\n",
    "\n",
    "    outputs = model(sequence_batch, cell_type_batch)\n",
    "\n",
    "    criterion.weight = target_mask_tr\n",
    "    loss = criterion(outputs, targets)\n",
    "    if criterion.reduction == \"sum\":\n",
    "        loss = loss / criterion.weight.sum()\n",
    "    predictions = torch.sigmoid(outputs)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return (\n",
    "        predictions.detach().numpy(),\n",
    "        targets.detach().numpy(),\n",
    "        target_mask_tr.numpy(),\n",
    "        loss.item(),\n",
    "    )\n",
    "\n",
    "def evaluate(model, batch, target_mask_tr):\n",
    "    \"\"\"\n",
    "    Makes predictions for some labeled input data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_in_batches : list(SamplesBatch)\n",
    "        A list of tuples of the data, where the first element is\n",
    "        the example, and the second element is the label.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple(float, list(numpy.ndarray))\n",
    "        Returns the average loss, and the list of all predictions.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sequence_batch = batch[0]#.to(device)\n",
    "    cell_type_batch = batch[1]#.to(device)\n",
    "    targets = batch[2]#.to(device)\n",
    "    target_mask = batch[3]#.to(device)\n",
    "    # print('targets', targets.shape)\n",
    "\n",
    "    # val mask\n",
    "    target_mask_val = target_mask.clone()\n",
    "    target_mask_val = ~target_mask_tr\n",
    "\n",
    "    if target_mask_val.shape[0] != targets.shape[0]:\n",
    "        target_mask_val = target_mask_val[:targets.shape[0], ...]\n",
    "\n",
    "    # compure a baseline\n",
    "    baseline = (targets * target_mask_val).sum(axis=1) / target_mask_val.sum(axis=1)\n",
    "    baseline = torch.repeat_interleave(baseline.unsqueeze(1), 631, dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sequence_batch, cell_type_batch)\n",
    "\n",
    "        criterion.weight = target_mask_val\n",
    "        loss = criterion(outputs, targets)\n",
    "        if criterion.reduction == \"sum\":\n",
    "            loss = loss / criterion.weight.sum()\n",
    "\n",
    "        predictions = torch.sigmoid(outputs)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        targets = targets.view(-1, targets.shape[-1])\n",
    "        baseline = baseline.view(-1, baseline.shape[-1])\n",
    "        target_mask = target_mask_val.view(-1, target_mask_val.shape[-1])\n",
    "\n",
    "    return loss, predictions, targets, baseline, target_mask\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.0001, weight_decay = 1e-6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from selene_sdk.utils import (\n",
    "    PerformanceMetrics,\n",
    "    initialize_logger,\n",
    "    load_model_from_state_dict,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "\n",
    "metrics=dict(roc_auc=roc_auc_score, average_precision=average_precision_score)\n",
    "\n",
    "_test_metrics = PerformanceMetrics(\n",
    "            lambda idx: train_batch_loader.dataset.dataset.target_features[idx],\n",
    "            report_gt_feature_n_positives=10,\n",
    "            metrics=metrics,\n",
    "        )\n",
    "\n",
    "print(train_batch_loader.dataset.dataset.target_features[0])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DNase-seq\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# train_batch_loader -- batches all samples in training folds.\n",
    "# valid_batch_loader -- batches all samples in validation fold.\n",
    "for fold, (train_batch_loader, valid_batch_loader) in enumerate(dataloaders):\n",
    "    print('fold:', fold, len(train_batch_loader), len(valid_batch_loader))\n",
    "    # Loop through all batches in training folds for a given split.\n",
    "    model.train()\n",
    "    tain_losses = []\n",
    "    for batch in tqdm(train_batch_loader):\n",
    "        # Train model on the training folds in the split.\n",
    "        prediction, target, target_mask, loss = train(model, batch, fold)\n",
    "        tain_losses.append(loss)\n",
    "    print('train loss:', np.average(tain_losses))\n",
    "\n",
    "    # Loop through all batches in validation fold for a given split.\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_target_masks = []\n",
    "    all_baselines = []\n",
    "    for batch in tqdm(valid_batch_loader):\n",
    "        # Test model on the validation fold in the split.\n",
    "        (\n",
    "            loss,\n",
    "            predictions,\n",
    "            targets,\n",
    "            baseline,\n",
    "            target_masks,\n",
    "        ) = evaluate(model, batch, target_mask_tr)\n",
    "\n",
    "        all_predictions.append(predictions.data.numpy())\n",
    "        all_targets.append(targets.data.numpy())\n",
    "        all_target_masks.append(target_masks.data.numpy())\n",
    "        all_baselines.append(baseline.data.numpy())\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "    all_predictions = expand_dims(np.concatenate(all_predictions))\n",
    "    all_targets = expand_dims(np.concatenate(all_targets))\n",
    "    all_baselines = expand_dims(np.concatenate(all_baselines))\n",
    "    all_target_masks = expand_dims(np.concatenate(all_target_masks))\n",
    "\n",
    "    # compute metrics\n",
    "    average_scores = _test_metrics.update(\n",
    "        all_predictions, all_targets, all_target_masks\n",
    "    )\n",
    "    baseline_score = _test_metrics.update(\n",
    "        all_baselines, all_targets, all_target_masks\n",
    "    )\n",
    "\n",
    "    for name, score in average_scores.items():\n",
    "        print(name, score)\n",
    "    for name, score in baseline_score.items():\n",
    "        print(f'baseline_{name}', score)   \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fold: 0 50 5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 50/50 [05:21<00:00,  6.43s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss: 0.6676654028892517\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:08<00:00,  1.76s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "roc_auc 0.8709891324896739\n",
      "average_precision 0.6355959132281161\n",
      "baseline_roc_auc 0.9642915112732995\n",
      "baseline_average_precision 0.7226254161902645\n",
      "fold: 1 50 5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 50/50 [05:26<00:00,  6.53s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss: 0.6404233521223068\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:06<00:00,  1.37s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "roc_auc 0.9067690793621503\n",
      "average_precision 0.6030687925927048\n",
      "baseline_roc_auc 0.9691030431640295\n",
      "baseline_average_precision 0.7024954911209768\n",
      "fold: 2 50 5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 28%|██▊       | 14/50 [01:31<03:55,  6.55s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-9e17b2bd6e4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Train model on the training folds in the split.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-3268baaf5103>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, batch, fold)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/genv/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/genv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('genv': venv)"
  },
  "interpreter": {
   "hash": "7bed6a5b8f0827f7d7f30ba9d7b2c61fcd2f223009d9afc95ff59e669a7c4ade"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}